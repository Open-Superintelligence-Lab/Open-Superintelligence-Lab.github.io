

## Open Tasks

[Does RoPE mess with semantics of the vectors, what would you do differently? ➝](/blog/rope-semantics)


[Make LLM pretraining accessible for everyone ➝](/blog/5-dollar-llm)




### Articles

### [Space State Model (SSM): Encode n-gram](/blog/ssm-encode-n-gram)
Claim: For any n-gram language model, there exists a state space language model that can simulate it with arbitrarily small error.

### [DeepSeek Sparse Attention](/blog/deepseek-sparse-attention)
Advanced research on DeepSeek's innovative sparse attention mechanisms for efficient long-context processing.

### [Tiny Recursive Model](/blog/tiny-recursive-model)
How a 7M parameter model beats 100x bigger models at Sudoku, Mazes, and ARC-AGI using recursive reasoning.

### [Pretrain LLM with NVFP4](/blog/pretrain-llm-with-nvfp4)
NVIDIA's breakthrough 4-bit training methodology achieving 2-3x speedup and 50% memory reduction.

### [47x Faster Image Generation](/blog/diffusion-transformer-representation-autoencoder)
Diffusion Transformers with Representation Autoencoders achieve state-of-the-art FID 1.13 on ImageNet.

### [QeRL: Beyond Efficiency](/blog/qerl-quantization-reinforcement-learning)
Quantization-enhanced Reinforcement Learning for LLMs enables RL training of 32B models on a single GPU.