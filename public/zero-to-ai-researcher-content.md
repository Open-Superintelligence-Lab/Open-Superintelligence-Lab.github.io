# Zero To AI Researcher - Full Course

## Introduction

The **Zero To AI Researcher Course** is a comprehensive educational journey that takes you from zero to AI researcher, focusing on cutting-edge transformer architectures including Mixture of Experts (MoE) models inspired by DeepSeek and GLM4.

This course provides hands-on experience with advanced AI research methodologies, from foundational concepts to implementing state-of-the-art architectures. Whether you're a beginner or looking to deepen your understanding of modern LLM research, this course offers a structured path to becoming an AI researcher.

## Key Questions This Course Answers

### 1. How do Mixture of Experts (MoE) models work and why are they crucial for scaling LLMs?

Learn the fundamentals of MoE architecture, expert routing mechanisms, and load balancing strategies that enable efficient scaling of language models while maintaining computational efficiency.

### 2. What makes DeepSeek's attention mechanisms innovative and how can they be combined with MoE?

Explore DeepSeek's advanced attention mechanisms, including sparse attention patterns, RoPE scaling, and LoRA-style projections, and understand how to integrate them with MoE architectures for optimal performance.

### 3. How do you conduct systematic AI research and evaluate different architectures?

Master the art of AI research through structured experimentation, ablation studies, benchmarking methodologies, and learn how to design and execute comprehensive research projects in the field of large language models.

## Course Structure

### Foundation Modules

- **Python fundamentals for AI research** - Essential programming skills for machine learning
- **Mathematical foundations** - Linear algebra, calculus, and statistics for deep learning
- **PyTorch basics and tensor operations** - Hands-on experience with the primary ML framework
- **Neural network fundamentals** - Understanding the building blocks of modern AI
- **Activation functions and optimization** - Key concepts for training neural networks

### Advanced Topics

- **Attention mechanisms and transformers** - The foundation of modern language models
- **DeepSeek attention implementation** - Advanced attention patterns and optimizations
- **GLM4 MoE architecture** - Mixture of Experts for efficient scaling
- **Hybrid model combinations** - Combining different architectures for optimal performance
- **Research methodology and evaluation** - Systematic approaches to AI research

## What You'll Build

### MoE Models
Implement efficient Mixture of Experts architectures with expert routing and load balancing. Learn to build models that can scale to billions of parameters while maintaining computational efficiency.

### DeepSeek Attention
Build advanced attention mechanisms with RoPE scaling and sparse patterns. Understand how to implement cutting-edge attention optimizations that reduce computational complexity.

### Research Framework
Develop systematic experimentation and evaluation methodologies. Learn to design comprehensive research projects, conduct ablation studies, and benchmark different architectures.

## Hands-On Experiments

### Experiment 1: Architecture Ablation Study
Compare 5 different model variants (baseline, MLP, attention+MLP, MoE, attention+MoE) to understand the impact of each component.

**Benchmark:** HellaSwag Benchmark

**Key Learning:** Understanding how different architectural choices affect model performance and efficiency.

### Experiment 2: Learning Rate Optimization
Systematic exploration of optimal learning rates for DeepSeek attention + MLP combinations.

**Method:** Grid Search

**Key Learning:** How to systematically optimize hyperparameters for different model architectures.

### Experiment 3: Expert Configuration Search
Optimize expert count, learning rates, and top-k values for DeepSeek attention + GLM4 MoE hybrid models.

**Evaluation:** Validation Metrics

**Key Learning:** Advanced optimization techniques for complex hybrid architectures.

## Getting Started

### 1. Clone the Repository
```bash
git clone https://github.com/vukrosic/zero-to-ai-researcher
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Start Learning
```bash
cd _course/01_python_beginner_lessons
```

## Course Modules Overview

### Module 1: Python Fundamentals
- Variables, data types, and control structures
- Functions and object-oriented programming
- NumPy and Pandas for data manipulation
- Matplotlib and Seaborn for visualization

### Module 2: Mathematical Foundations
- Linear algebra essentials for deep learning
- Calculus and optimization theory
- Probability and statistics
- Information theory basics

### Module 3: PyTorch Deep Dive
- Tensor operations and autograd
- Building neural networks from scratch
- Training loops and optimization
- GPU acceleration and distributed training

### Module 4: Transformer Architecture
- Self-attention mechanism
- Multi-head attention
- Positional encoding
- Layer normalization and residual connections

### Module 5: Advanced Architectures
- Mixture of Experts (MoE) models
- Sparse attention mechanisms
- Memory-efficient attention
- Hybrid architectures

### Module 6: Research Methodology
- Experimental design
- Ablation studies
- Benchmarking and evaluation
- Paper writing and presentation

## Prerequisites

### Technical Requirements
- Basic programming experience (any language)
- High school level mathematics
- Access to a computer with GPU (recommended)
- 10-15 hours per week for 12 weeks

### Recommended Background
- Some exposure to machine learning concepts
- Familiarity with Python (helpful but not required)
- Interest in AI research and development

## Learning Outcomes

By the end of this course, you will:

1. **Understand** the mathematical foundations of modern AI architectures
2. **Implement** transformer models from scratch using PyTorch
3. **Build** MoE models with efficient expert routing
4. **Design** and conduct systematic AI research experiments
5. **Evaluate** different architectures using proper benchmarking techniques
6. **Contribute** to open-source AI research projects

## Community and Support

### Discord Community
Join our active Discord server to connect with fellow learners, ask questions, and share your progress.

### Office Hours
Weekly office hours with course instructors and teaching assistants.

### Peer Learning
Collaborate with other students on projects and research experiments.

## Career Pathways

This course prepares you for various roles in AI research and development:

- **AI Research Scientist** - Conducting cutting-edge research in machine learning
- **ML Engineer** - Building and deploying AI systems in production
- **Research Engineer** - Implementing research ideas and optimizing models
- **AI Consultant** - Advising organizations on AI strategy and implementation

## Resources and References

### Essential Papers
- "Attention Is All You Need" - Vaswani et al.
- "Switch Transformer: Scaling to Trillion Parameter Models" - Fedus et al.
- "DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency" - DeepSeek Team
- "GLM-4: Advancing Multilingual and Multimodal Capabilities" - GLM Team

### Recommended Books
- "Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
- "Pattern Recognition and Machine Learning" by Christopher Bishop
- "The Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman

### Online Resources
- PyTorch Documentation and Tutorials
- Hugging Face Transformers Library
- Papers With Code for latest research implementations

## Assessment and Certification

### Course Projects
- **Project 1:** Implement a basic transformer from scratch
- **Project 2:** Build and optimize an MoE model
- **Project 3:** Conduct a comprehensive ablation study
- **Final Project:** Original research project with paper submission

### Certification
Upon successful completion of all modules and projects, you'll receive a certificate of completion from Open Superintelligence Lab.

## Frequently Asked Questions

### Q: Do I need a GPU to take this course?
A: While not strictly required, a GPU will significantly speed up training and experimentation. We provide cloud computing credits for students who need them.

### Q: How much time should I expect to spend per week?
A: Plan for 10-15 hours per week, including lectures, hands-on exercises, and project work.

### Q: Is this course suitable for complete beginners?
A: Yes! The course starts with Python fundamentals and builds up to advanced concepts. No prior AI experience is required.

### Q: Can I work at my own pace?
A: The course is designed to be completed in 12 weeks, but you can work at your own pace within the semester.

### Q: What kind of support is available?
A: We provide Discord community support, weekly office hours, and peer learning opportunities.

## Ready to Start Your AI Research Journey?

Join the open-source community and contribute to cutting-edge AI research. This course provides the foundation you need to become an AI researcher and make meaningful contributions to the field.

### Next Steps
1. **Enroll** in the course through our GitHub repository
2. **Join** our Discord community for support and networking
3. **Start** with Module 1: Python Fundamentals
4. **Connect** with fellow learners and instructors

### Get Involved
- **Contribute** to open-source AI projects
- **Share** your research findings with the community
- **Mentor** future students
- **Collaborate** on cutting-edge research

---

*This course is part of the Open Superintelligence Lab's mission to democratize AI research and education. We believe that everyone should have access to the tools and knowledge needed to contribute to the future of artificial intelligence.*
