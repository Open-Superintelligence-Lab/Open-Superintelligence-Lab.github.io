---
hero:
  title: "The Concept of Learning"
  subtitle: "How Neurons Adjust Their Weights"
  tags:
    - "ðŸ§  Neuron"
    - "â±ï¸ 8 min read"
---

Learning is the process of **adjusting weights to reduce loss**. The neuron literally learns from mistakes!

![Learning Process](/content/learn/neuron-from-scratch/the-concept-of-learning/learning-process.png)

## The Mathematical Foundation of Learning

Machine learning is fundamentally an **optimization problem**. We want to find parameters (weights and biases) that minimize a loss function.

**Mathematically:**
```
Find Î¸* = argmin L(Î¸)
          Î¸

Where:
  Î¸ = model parameters (weights and biases)
  L(Î¸) = loss function
  Î¸* = optimal parameters that minimize loss
  argmin = "argument that minimizes"
```

### Why Gradient Descent?

For complex neural networks, we can't solve this analytically. Instead, we use **gradient descent**: an iterative algorithm that follows the slope downhill.

**Geometric intuition:**
- Loss function creates a "landscape" over parameter space
- We start at a random point (random weights)
- We look at the slope (gradient)
- We take a step downhill (opposite to gradient)
- Repeat until we reach a minimum!

## What Does "Learning" Mean?

**Learning = Automatically adjusting weights to make better predictions**

### The Calculus Behind Learning

**Gradient**: The vector of partial derivatives showing how loss changes with each parameter.

```
âˆ‡L(Î¸) = [âˆ‚L/âˆ‚Î¸â‚, âˆ‚L/âˆ‚Î¸â‚‚, ..., âˆ‚L/âˆ‚Î¸â‚™]

Where:
  âˆ‡ (nabla) = gradient operator
  âˆ‚L/âˆ‚Î¸áµ¢ = partial derivative of loss w.r.t. parameter i
```

**Interpretation:**
- If âˆ‚L/âˆ‚Î¸áµ¢ > 0: increasing Î¸áµ¢ increases loss â†’ decrease Î¸áµ¢
- If âˆ‚L/âˆ‚Î¸áµ¢ < 0: increasing Î¸áµ¢ decreases loss â†’ increase Î¸áµ¢
- Magnitude |âˆ‚L/âˆ‚Î¸áµ¢| shows how sensitive loss is to Î¸áµ¢

```yaml
Before learning:
  Weights: Random
  Predictions: Bad
  Loss: High

After learning:
  Weights: Optimized
  Predictions: Good
  Loss: Low
```

## The Learning Process

**Step-by-step:**

1. Make prediction (forward pass)
2. Calculate loss (how wrong?)
3. Calculate gradients (which direction to adjust?)
4. Update weights (move in right direction)
5. Repeat!

**Example:**

```python
import torch
import torch.nn as nn

# Model
model = nn.Linear(1, 1)

# Training data
x = torch.tensor([[1.0], [2.0], [3.0]])
y = torch.tensor([[2.0], [4.0], [6.0]])  # y = 2x

# Loss function
criterion = nn.MSELoss()

# Optimizer (handles weight updates)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Training loop
for epoch in range(100):
    # 1. Forward pass
    predictions = model(x)
    
    # 2. Calculate loss
    loss = criterion(predictions, y)
    
    # 3. Backward pass (calculate gradients)
    optimizer.zero_grad()
    loss.backward()
    
    # 4. Update weights
    optimizer.step()
    
    if epoch % 20 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

# After training
print(f"Learned weight: {model.weight.item():.2f}")  # Should be close to 2.0
print(f"Learned bias: {model.bias.item():.2f}")      # Should be close to 0.0
```

## Gradient Descent

**The algorithm that powers learning.**

### Mathematical Definition

**Update rule:**
```
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· âˆ‡L(Î¸â‚œ)

Where:
  Î¸â‚œ = parameters at step t
  Î¸â‚œâ‚Šâ‚ = updated parameters
  Î· (eta) = learning rate (step size)
  âˆ‡L(Î¸â‚œ) = gradient of loss at current parameters
```

**Step-by-step:**
1. Compute loss: L(Î¸â‚œ)
2. Compute gradient: âˆ‡L(Î¸â‚œ) = [âˆ‚L/âˆ‚Î¸â‚, âˆ‚L/âˆ‚Î¸â‚‚, ...]
3. Update each parameter: Î¸áµ¢ â† Î¸áµ¢ - Î·(âˆ‚L/âˆ‚Î¸áµ¢)
4. Repeat!

### Example Calculation

Consider a single weight w:

```
Current state:
  w = 0.5
  L(w) = (Å· - y)Â² where Å· = wx
  
Suppose:
  x = 2.0, y = 3.0 (true value)
  Å· = 0.5 Ã— 2.0 = 1.0 (prediction)
  L = (1.0 - 3.0)Â² = 4.0 (high loss!)

Gradient calculation:
  âˆ‚L/âˆ‚w = âˆ‚/âˆ‚w[(wx - y)Â²]
        = 2(wx - y) Â· x         (chain rule)
        = 2(1.0 - 3.0) Â· 2.0
        = -8.0

Update (Î· = 0.1):
  w_new = w - Î·(âˆ‚L/âˆ‚w)
        = 0.5 - 0.1Ã—(-8.0)
        = 0.5 + 0.8
        = 1.3

Check: New prediction = 1.3 Ã— 2.0 = 2.6 (closer to 3.0!)
```

**Worked example:**

```yaml
Current weight: w = 0.5
Loss: high

Gradient: âˆ‚Loss/âˆ‚w = -2.3
  Negative gradient â†’ loss decreases if we INCREASE w

Update:
  w_new = w - learning_rate Ã— gradient
  w_new = 0.5 - 0.01 Ã— (-2.3)
  w_new = 0.5 + 0.023
  w_new = 0.523

Result: Loss is now lower!
```

### The Mathematics of Convergence

**Why does gradient descent work?**

By Taylor expansion, for small Î·:
```
L(Î¸ - Î·âˆ‡L) â‰ˆ L(Î¸) - Î·||âˆ‡L||Â² + O(Î·Â²)
```

Since ||âˆ‡L||Â² > 0, if Î· is small enough:
```
L(Î¸ - Î·âˆ‡L) < L(Î¸)
```

So each step reduces the loss! (Assuming Î· is not too large)

**Convergence conditions:**
- Loss is bounded below
- Gradients are Lipschitz continuous
- Learning rate satisfies: Î£Î·â‚œ = âˆž, Î£Î·â‚œÂ² < âˆž

## Learning Rate

**Learning rate controls how big each step is.**

### Mathematical Analysis

The learning rate Î· is a hyperparameter that balances:
- **Speed**: Larger Î· â†’ faster convergence
- **Stability**: Smaller Î· â†’ more stable, less oscillation

**Optimal learning rate theorem:**
For quadratic loss L(w) = Â½w^T Aw:
```
Optimal Î· = 2/(Î»_min + Î»_max)

Where Î»_min, Î»_max are smallest/largest eigenvalues of A
```

In practice, we don't know this, so we:
- Start with Î· â‰ˆ 0.01 or 0.001
- Use adaptive optimizers (Adam, RMSprop)
- Use learning rate schedules

```python
# Too small: slow learning
optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)
# Takes forever to learn!

# Just right: good learning
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
# Learns efficiently

# Too large: unstable learning
optimizer = torch.optim.SGD(model.parameters(), lr=10.0)
# Might overshoot and never converge!
```

**Effect of learning rate:**

```yaml
lr = 0.001 (small):
  Small weight updates
  Slow but stable
  Many epochs needed

lr = 0.01 (medium):
  Moderate updates
  Good balance
  Converges reasonably

lr = 1.0 (large):
  Large weight updates
  Fast but unstable
  Might oscillate or diverge
```

### Mathematical Effects of Learning Rate

**Too small (Î· = 0.0001):**
```
Î”Î¸ = -Î·âˆ‡L = very small
â†’ Many iterations needed
â†’ May not reach minimum in reasonable time
```

**Just right (Î· = 0.01):**
```
Î”Î¸ = -Î·âˆ‡L = appropriate size
â†’ Steady progress
â†’ Converges efficiently
```

**Too large (Î· = 10):**
```
Î”Î¸ = -Î·âˆ‡L = too large
â†’ Overshoots minimum
â†’ Loss oscillates or diverges
```

## Simple Learning Example

Let's derive and implement gradient descent from first principles!

### Mathematical Setup

We want to learn the linear relationship y = wx + b:

```python
import torch

# True relationship: y = 3x + 1
x_train = torch.tensor([1.0, 2.0, 3.0, 4.0])
y_train = torch.tensor([4.0, 7.0, 10.0, 13.0])

# Model (start with random weights)
w = torch.tensor([0.5], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

learning_rate = 0.01

# Train for 100 steps
for step in range(100):
    # Prediction
    y_pred = w * x_train + b
    
    # Loss
    loss = ((y_pred - y_train) ** 2).mean()
    
    # Backpropagation
    loss.backward()
    
    # Update weights
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad
        
        # Reset gradients
        w.grad.zero_()
        b.grad.zero_()
    
    if step % 20 == 0:
        print(f"Step {step}: w={w.item():.2f}, b={b.item():.2f}, loss={loss.item():.4f}")

print(f"\\nLearned: y = {w.item():.2f}x + {b.item():.2f}")
# Should be close to: y = 3x + 1
```

### Detailed Gradient Derivation

Let's derive the gradients manually:

**Loss function:**
```
L = (1/n)Î£áµ¢(Å·áµ¢ - yáµ¢)Â²
where Å·áµ¢ = wxáµ¢ + b
```

**Gradient w.r.t. w:**
```
âˆ‚L/âˆ‚w = âˆ‚/âˆ‚w[(1/n)Î£áµ¢(wxáµ¢ + b - yáµ¢)Â²]
      = (1/n)Î£áµ¢ 2(wxáµ¢ + b - yáµ¢) Â· xáµ¢    (chain rule)
      = (2/n)Î£áµ¢(wxáµ¢ + b - yáµ¢)xáµ¢
```

**Gradient w.r.t. b:**
```
âˆ‚L/âˆ‚b = âˆ‚/âˆ‚b[(1/n)Î£áµ¢(wxáµ¢ + b - yáµ¢)Â²]
      = (1/n)Î£áµ¢ 2(wxáµ¢ + b - yáµ¢) Â· 1      (chain rule)
      = (2/n)Î£áµ¢(wxáµ¢ + b - yáµ¢)
```

**Update rules:**
```
w â† w - Î·(âˆ‚L/âˆ‚w)
b â† b - Î·(âˆ‚L/âˆ‚b)
```

These are exactly what PyTorch computes with `loss.backward()`!

## What the Neuron Learns

### Feature Learning

```python
# Example: Learning to classify

# Initially (random weights):
prediction = neuron([1.0, 2.0])  # 0.34 (wrong!)
actual = 1.0
loss = high

# After seeing examples:
# The neuron learns that:
# - Feature 1 with value > 0.5 â†’ usually class 1
# - Feature 2 with value > 1.0 â†’ usually class 1
# So it adjusts weights accordingly

# Finally (trained weights):
prediction = neuron([1.0, 2.0])  # 0.98 (correct!)
actual = 1.0
loss = low
```

## Key Takeaways

âœ“ **Learning = Adjusting weights:** Based on errors

âœ“ **Goal:** Minimize loss

âœ“ **Gradient descent:** The learning algorithm

âœ“ **Learning rate:** Controls step size

âœ“ **Automatic:** PyTorch calculates gradients for you!

**Quick Reference:**

```python
# Training loop
for epoch in range(num_epochs):
    # Forward pass
    predictions = model(inputs)
    
    # Calculate loss
    loss = criterion(predictions, targets)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    
    # Update weights
    optimizer.step()
```

**Remember:** Learning is just: predict â†’ measure error â†’ adjust â†’ repeat! ðŸŽ‰
