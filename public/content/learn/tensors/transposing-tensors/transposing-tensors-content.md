---
hero:
  title: "Transposing Tensors"
  subtitle: "Flipping Dimensions and Axes"
  tags:
    - "üî¢ Tensors"
    - "‚è±Ô∏è 10 min read"
---

Transposing is like **flipping** a tensor - rows become columns, and columns become rows. It's simple but incredibly useful!

## The Basic Idea

**Transpose = Swap rows and columns**

Think of it like rotating a table 90 degrees. The first row becomes the first column, the second row becomes the second column, and so on.

## Vector Transpose

When you transpose a vector, you change it from horizontal to vertical (or vice versa):

![Vector Transpose](/content/learn/tensors/transposing-tensors/vector-transpose.png)

**Example:**

```python
import torch

# Horizontal vector (row)
v = torch.tensor([1, 2, 3, 4])
print(v.shape)  # torch.Size([4])

# Transpose to vertical (column)
v_t = v.T
print(v_t)
# tensor([[1],
#         [2],
#         [3],
#         [4]])
print(v_t.shape)  # torch.Size([4, 1])
```

**Manual visualization:**

```yaml
Before: [1, 2, 3, 4]  ‚Üí  Shape: (4,)

After:  [[1],
         [2],
         [3],
         [4]]          ‚Üí  Shape: (4, 1)
```

## Matrix Transpose

This is where transpose really shines! Rows become columns, columns become rows:

![Matrix Transpose](/content/learn/tensors/transposing-tensors/matrix-transpose.png)

**Example:**

```python
import torch

# Original matrix: 2 rows, 3 columns
A = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])

print(A.shape)  # torch.Size([2, 3])

# Transpose: 3 rows, 2 columns
A_T = A.T

print(A_T)
# tensor([[1, 4],
#         [2, 5],
#         [3, 6]])

print(A_T.shape)  # torch.Size([3, 2])
```

**Manual calculation:**

```yaml
Original (2√ó3):
[[1, 2, 3],
 [4, 5, 6]]

Transpose (3√ó2):
[[1, 4],    ‚Üê First column becomes first row
 [2, 5],    ‚Üê Second column becomes second row
 [3, 6]]    ‚Üê Third column becomes third row
```

## How Elements Move

Here's exactly what happens to each element during transpose:

![Transpose Detailed](/content/learn/tensors/transposing-tensors/transpose-detailed.png)

**The pattern:** Position `[i, j]` ‚Üí Position `[j, i]`

**Example tracking specific elements:**

```yaml
Original position ‚Üí Transposed position

[0, 0]: value 1  ‚Üí  [0, 0]: value 1  (stays in place)
[0, 1]: value 2  ‚Üí  [1, 0]: value 2  (row 0, col 1 ‚Üí row 1, col 0)
[0, 2]: value 3  ‚Üí  [2, 0]: value 3
[1, 0]: value 4  ‚Üí  [0, 1]: value 4
[1, 1]: value 5  ‚Üí  [1, 1]: value 5  (stays in place)
[1, 2]: value 6  ‚Üí  [2, 1]: value 6
```

**Key rule:** Just swap the two indices! `[i, j]` becomes `[j, i]`

## Square Matrix Transpose

Square matrices (same number of rows and columns) have a special property:

![Square Transpose](/content/learn/tensors/transposing-tensors/square-transpose.png)

**Example:**

```python
import torch

A = torch.tensor([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]])

print(A.shape)  # torch.Size([3, 3])

A_T = A.T
print(A_T)
# tensor([[1, 4, 7],
#         [2, 5, 8],
#         [3, 6, 9]])

print(A_T.shape)  # torch.Size([3, 3])
```

**What happens:**

```yaml
Original:           Transposed:
[[1, 2, 3],        [[1, 4, 7],
 [4, 5, 6],   ‚Üí     [2, 5, 8],
 [7, 8, 9]]         [3, 6, 9]]

Diagonal (1, 5, 9) stays in place!
Everything else flips across the diagonal.
```

**The diagonal stays put:** Elements where row = column don't move!

## Shape Changes

The shape always flips:

```python
# Examples of shape changes
original_shape = (2, 3)
transposed_shape = (3, 2)

original_shape = (5, 7)
transposed_shape = (7, 5)

original_shape = (4, 4)  # Square
transposed_shape = (4, 4)  # Still square!
```

**Quick reference:**

```yaml
(2, 3) ‚Üí (3, 2)
(5, 1) ‚Üí (1, 5)
(10, 20) ‚Üí (20, 10)
(n, m) ‚Üí (m, n)  ‚Üê General pattern
```

## Why Do We Transpose?

The most common reason: **making shapes compatible for matrix multiplication!**

![Why Transpose](/content/learn/tensors/transposing-tensors/why-transpose.png)

**Example:**

```python
import torch

A = torch.randn(2, 3)  # Shape: (2, 3)
B = torch.randn(2, 4)  # Shape: (2, 4)

# This WON'T work - shapes incompatible
# result = A @ B  # Error! 3 ‚â† 2

# Transpose B to make it work!
B_T = B.T  # Shape: (4, 2)

# Now this works!
result = A @ B_T  # (2, 3) @ (4, 2)? Wait, still wrong!

# Actually, we need different dimensions
# Let's try a real example:
A = torch.randn(2, 3)
B = torch.randn(4, 3)  # Same inner dimension as A's columns

# Without transpose - doesn't work
# result = A @ B  # Error! (2,3) @ (4,3) - 3 ‚â† 4

# With transpose - works!
result = A @ B.T  # (2,3) @ (3,4) = (2,4) ‚úì

print(result.shape)  # torch.Size([2, 4])
```

**Real example with actual values:**

```python
import torch

# Two data samples with 3 features each
X = torch.tensor([[1.0, 2.0, 3.0],
                  [4.0, 5.0, 6.0]])  # Shape: (2, 3)

# Weight matrix: 3 inputs, 2 outputs (we want this orientation)
W = torch.tensor([[0.1, 0.2],
                  [0.3, 0.4],
                  [0.5, 0.6]])  # Shape: (3, 2)

# This works!
output = X @ W  # (2, 3) @ (3, 2) = (2, 2)
print(output)
# tensor([[2.2000, 2.8000],
#         [4.9000, 6.4000]])

# But if W was stored transposed...
W_stored = W.T  # Shape: (2, 3)

# We need to transpose it back
output = X @ W_stored.T  # (2, 3) @ (3, 2) = (2, 2)
print(output)  # Same result!
```

## Double Transpose

Transposing twice brings you back to the original:

![Double Transpose](/content/learn/tensors/transposing-tensors/double-transpose.png)

**Example:**

```python
import torch

A = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])

A_T = A.T      # First transpose
A_T_T = A_T.T  # Second transpose

print(torch.equal(A, A_T_T))  # True - back to original!
```

**Why this matters:**

```yaml
Original:  (2, 3)
.T:        (3, 2)  ‚Üê Different
.T.T:      (2, 3)  ‚Üê Back to original!

Pattern: A.T.T = A
```

## Practical Examples

### Example 1: Computing Dot Products

```python
import torch

# Two vectors
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

# Can't use @ directly on 1D tensors for matrix multiply
# But we can reshape and transpose!

a_col = a.reshape(-1, 1)  # Column vector (3, 1)
b_row = b.reshape(1, -1)  # Row vector (1, 3)

# Outer product
outer = a_col @ b_row  # (3, 1) @ (1, 3) = (3, 3)
print(outer)
# tensor([[ 4,  5,  6],
#         [ 8, 10, 12],
#         [12, 15, 18]])

# Inner product (dot product)
inner = b_row @ a_col  # (1, 3) @ (3, 1) = (1, 1)
print(inner)  # tensor([[32]])
```

### Example 2: Batch Matrix Transpose

```python
import torch

# Batch of 3 matrices, each 2√ó4
batch = torch.randn(3, 2, 4)

# Transpose last two dimensions
batch_T = batch.transpose(-2, -1)  # Now (3, 4, 2)

print(batch.shape)    # torch.Size([3, 2, 4])
print(batch_T.shape)  # torch.Size([3, 4, 2])

# Each of the 3 matrices got transposed individually!
```

### Example 3: Neural Network Weights

```python
import torch

# In neural networks, weights are often stored transposed
# for computational efficiency

batch_size = 32
input_features = 10
output_features = 5

# Input batch
X = torch.randn(batch_size, input_features)  # (32, 10)

# Weights stored as (input, output) for efficiency
W = torch.randn(input_features, output_features)  # (10, 5)

# Forward pass - works directly!
output = X @ W  # (32, 10) @ (10, 5) = (32, 5) ‚úì

# If weights were stored as (output, input) instead...
W_alt = torch.randn(output_features, input_features)  # (5, 10)

# Need to transpose
output = X @ W_alt.T  # (32, 10) @ (10, 5) = (32, 5) ‚úì
```

## Common Gotchas

### ‚ùå Gotcha 1: 1D Tensors Don't Change Much

```python
v = torch.tensor([1, 2, 3])
v_t = v.T

print(torch.equal(v, v_t))  # True!
# 1D tensors look the same after transpose!
```

To actually change a 1D tensor, reshape it first:

```python
v = torch.tensor([1, 2, 3])
v_col = v.reshape(-1, 1)  # Column vector

print(v.shape)      # torch.Size([3])
print(v_col.shape)  # torch.Size([3, 1])
```

### ‚ùå Gotcha 2: Transpose Creates a View

```python
A = torch.tensor([[1, 2], [3, 4]])
A_T = A.T

# Modifying A_T also modifies A!
A_T[0, 0] = 999

print(A)
# tensor([[999,   2],
#         [  3,   4]])

# Use .clone() if you want a copy
A_T_copy = A.T.clone()
A_T_copy[0, 0] = 42
# A is unchanged
```

## Key Takeaways

‚úì **Transpose swaps rows and columns:** `[i, j]` ‚Üí `[j, i]`

‚úì **Shape flips:** `(m, n)` ‚Üí `(n, m)`

‚úì **Double transpose returns original:** `A.T.T = A`

‚úì **Main use:** Making shapes compatible for matrix multiplication

‚úì **Diagonal stays:** In square matrices, diagonal elements don't move

‚úì **Use `.T`:** Simple and clean syntax in PyTorch

**Quick Reference:**

```python
# Basic transpose
A = torch.tensor([[1, 2, 3], [4, 5, 6]])
A_T = A.T  # Shape: (2,3) ‚Üí (3,2)

# For 3D+ tensors, specify dimensions
B = torch.randn(5, 10, 20)
B_T = B.transpose(1, 2)  # Swap dimensions 1 and 2
# Shape: (5, 10, 20) ‚Üí (5, 20, 10)

# Transpose last two dimensions (common in batch operations)
C = torch.randn(8, 4, 6)
C_T = C.transpose(-2, -1)  # or C.transpose(1, 2)
# Shape: (8, 4, 6) ‚Üí (8, 6, 4)
```

**Remember:** Transposing is just flipping! Rows ‚Üí Columns, Columns ‚Üí Rows. That's it! üéâ
