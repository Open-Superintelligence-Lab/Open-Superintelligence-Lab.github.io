---
hero:
  title: "The Feedforward Layer"
  subtitle: "FFN in Transformer Blocks"
  tags:
    - "üîÄ MoE"
    - "‚è±Ô∏è 8 min read"
---

The feedforward network (FFN) in transformers processes each position independently!

## Structure

```python
import torch.nn as nn

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        return self.net(x)

# Typical: d_ff = 4 √ó d_model
ffn = FeedForward(d_model=512, d_ff=2048)
```

## Key Takeaways

‚úì **Two layers:** Expand then compress

‚úì **Position-wise:** Same FFN for each position

‚úì **Standard ratio:** d_ff = 4 √ó d_model

**Remember:** FFN adds capacity after attention! üéâ
