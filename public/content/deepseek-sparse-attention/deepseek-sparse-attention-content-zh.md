---
hero:
  title: "DeepSeek 稀疏注意力"
  subtitle: "⚡ 从二次复杂度到近线性注意力——闪电索引器的突破"
  tags:
    - "⏱️ 技术深度解析"
    - "📄 研究文章"
---

[研究论文](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf) • [模型](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp) • [GitHub](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp) • [加入 DeepSeek 稀疏注意力的开源研究](https://github.com/Open-Superintelligence-Lab/deepseek-sparse-attention-research)

> **🚦 前置知识：**  
> 为了更好地理解本文，你需要对**注意力机制**有基本了解。  
>  
> _如果你还不清楚什么是注意力机制？请滚动到文末的**推荐视频资源**快速入门！_

新的 DeepSeek 架构让原本已经很快（BRR）的大语言模型变得更快（BRRRRRRRRRRRR）——它将注意力复杂度从二次增长 $O(L^2)$ 降低到近线性增长 $O(L \cdot k)$，其中 $L$ 是序列长度（上下文窗口中的 token 数量），$k$ 是一个较小的常数（例如 2048）。

![推理成本对比图](/content/deepseek-sparse-attention/Inference-cost.jpeg)

DeepSeek-V3.2-Exp 通过稀疏注意力显著降低了推理和训练成本，在保持性能的同时大幅节省计算资源。

> ⚡ **开发者注意：**  
> 这很可能是一个早期信号，预示着正在构建基础设施的开发者们应**为 DeepSeek-V4 做好准备**，后者很可能采用相同的注意力机制。

**📺 推荐视频资源：** 想全面理解注意力机制和 DeepSeek 的多头潜在注意力（MLA），请观看我的课程：[《从零开始理解 DeepSeek V3》](https://youtu.be/TfEG0TwueTs)

-  **如果你是注意力机制的新手：** 请从视频开头看到 58:39（编码部分开始前）。编码部分可选观看。
-  **如果你已理解经典注意力机制，只想了解 DeepSeek 的多头潜在注意力（MLA）：** 请从 38:53 开始观看，或直接点击此链接：[https://youtu.be/TfEG0TwueTs?t=2333](https://youtu.be/TfEG0TwueTs?t=2333)
-  **注意：** 我将在本文/视频中再次解释 MLA，但建议两者都看以获得更深入的理解。

💡 *我们也在对该主题进行研究——请参见文章底部的研究成果。*

标准 Transformer 使用一种“注意力”机制，其中每个新生成的 token 都会回看序列中所有之前的 token。

这种机制计算开销极大。如果序列长度为 $L$，其复杂度为 $O(L^2)$，意味着计算和内存需求呈二次方增长。

例如，将文本长度从 10,000 个 token 增加到 20,000 个 token，并不会仅仅使成本翻倍，而是使其变为原来的四倍。这使得处理非常长的文档（如整本书或大型代码库）变得极其缓慢且昂贵。

DeepSeek 稀疏注意力（DSA）不再让每个 token 关注所有之前的 token，而是智能地选择一个固定大小的小子集（$k$）中最相关的先前 token 进行关注。这将复杂度从 $O(L^2)$ 降低到 $O(L \cdot k)$，由于 $k$ 是一个小常数（例如 2048），而 $L$ 可以非常大（例如 128,000 或 2,000,000），因此这种复杂度要容易管理得多。

![注意力架构图](/content/deepseek-sparse-attention/Attention-architecture.png)

*接下来我们将解释 DSA（图中绿色部分）如何与 MLA（多头潜在注意力）协同工作。*

DSA 由两个主要组件构成：

**闪电索引器（Lightning Indexer）** 本质上是一种经典注意力机制，但体积更小、速度更快。它在所有 token 之间执行完整注意力计算，但使用 ReLU 激活函数（计算速度极快）以及更小维度的键（key）和查询（query）。

#### 组件一：闪电索引器

这是一个快速轻量的机制，其唯一任务是判断哪些历史 token 对当前 token 最重要。它的工作方式与标准注意力完全相同：通过计算 token 之间的查询（q）和键（k）的点积来生成注意力分数。然而，与标准注意力不同的是，闪电索引器并不使用这些分数来融合其他 token 的信息或上下文，而是纯粹用它们来判断 token 之间的“重要性关系”。分数 = 重要性。模型会学习生成能够准确计算这些重要性分数的键和查询。

-  **工作原理：** 对于当前 token（$h_t$，其中 $t$ 表示当前 token 的索引），索引器会快速为每个历史 token（$h_s$）计算一个“索引分数”（$I_{t,s}$，其中 $t$ 是当前 token，$s$ 是历史 token）。该分数表示 token $s$ 对 token $t$ 的预测相关性。
-  **公式 (1)：** 公式 1 本质上是一种简化的注意力计算。它使用自己的一组小型查询（$q^I$）和键（$k^I$）来计算这些分数（上标 $I$ 表示属于索引器计算）。
-  **为何称为“闪电”：** 它专为速度而设计。它使用简单的 $\text{ReLU}$ 激活函数，并可使用低精度数值（FP8）运行，使其计算成本极低，即使它在技术上仍需查看所有历史 token（即 $O(L^2)$ 操作，但速度极快）。

### 1. 公式详解（“是什么”）

论文提供了两个关键公式来描述这一两阶段过程。

#### **公式 (1)：闪电索引器**

$$
I_{t,s} = \sum_{j=1}^{H_I} w_{t,j}^I \cdot \text{ReLU}(q_{t,j}^I \cdot k_s^I)
$$

*注：$j$ 表示闪电索引器中的注意力头索引，范围从 1 到总头数 $H_I$。*

该公式计算**索引分数**（$I_{t,s}$），表示历史 token $s$ 对当前 token $t$ 的“相关性”。我们来逐项解析：

*   $I_{t,s}$：最终的重要性分数。分数越高，表示 token $s$ 对 token $t$ 越重要。
*   $h_t$ 和 $h_s$：分别是当前 token（$t$）和历史 token（$s$）的向量表示（隐藏状态）。
*   $q_{t,j}^I$ 和 $k_s^I$：这是专为索引器设计的轻量级**查询**和**键**向量（由上标 $I$ 标识）。它们分别由 $h_t$ 和 $h_s$ 推导而来。
*   $q_{t,j}^I \cdot k_s^I$：点积运算，是注意力机制的核心操作，用于衡量查询与键之间的相似性或兼容性。
*   $\text{ReLU}(\cdots)$：一种简单的激活函数（修正线性单元）。计算速度极快：若点积为负，则输出 0；否则保持原值。
*   $w_{t,j}^I$：一个额外的权重，同样由当前 token $h_t$ 推导而来。它作为每个索引器头 $j$ 的可学习门控或重要性因子。
*   $\sum \cdots$：对所有索引器头（$H^I$）的结果求和。索引器仅包含少量注意力头以保证速度。

**通俗解释：** 闪电索引器是一个微型、简化的注意力机制。它的唯一任务是快速计算每对 token 之间的相关性分数，而无需执行完整且昂贵的注意力计算。

#### **公式 (2)：主注意力计算**

$$
u_t = \text{Attn}(h_t, \{c_s | I_{t,s} \in \text{Top-k}(I_{t,:})\})
$$

该公式展示了在选出最相关 token 后如何计算最终输出（$u_t$）。（$u_t$）与标准注意力机制输出相同——即融合了历史 token 上下文信息的当前 token 向量嵌入。

[注意力机制详解](https://youtu.be/wcDV3l4CD14?t=5562)

与标准注意力一样，$u_t$ 将当前 token 与历史 token 的上下文信息结合——但不再关注所有历史 token，而仅整合闪电索引器确定的 Top-k 个最重要 token 的上下文。

*   $u_t$：当前 token $t$ 的最终输出隐藏状态。
*   $\text{Attn}(\cdots)$：表示主注意力机制（此处为多查询注意力，Multi-Query Attention）。
*   $h_t$：（此处略有混淆）表示当前 token 的查询（query）。在前一个公式中，$h_t$ 是当前 token 本身的向量表示（隐藏状态），而此处是其查询向量。
*   $\{c_s | I_{t,s} \in \text{Top-k}(I_{t,:})\}$：这是最关键的部分。其含义是：“仅当索引分数 $I_{t,s}$（由公式 1 计算得出）在当前 token $t$ 的所有分数中排名前 $k$ 时，才使用对应的键值对 $c_s$。”

**通俗解释：** 主注意力机制被告知忽略几乎所有历史 token，仅关注闪电索引器识别出的少数（例如 2,048 个）最关键键值对。

#### 组件二：细粒度 Token 选择

该组件非常简单：它接收闪电索引器计算出的所有索引分数，并选出其中最高的 $\text{top-k}$ 个。

-  **功能：** 它充当守门人角色，告诉主注意力机制：“你不需要查看全部 100,000 个历史 token。我已经为你找出了其中最重要的 2,048 个。只需关注这些即可。”

最终的注意力输出（$u_t$）由主注意力模块计算得出，但仅使用当前 token 的查询和索引器选出的 $top-k$ 个键值对。

### 第三步：模型训练方式

DeepSeek-V3.2-Exp 的创建并非从零开始，而是巧妙地改造了一个已有的强大模型——**DeepSeek-V3.1-Terminus**。该模型原本就擅长处理长达 128K token 的长上下文。此次改造采用多阶段训练流程，旨在无缝集成新的稀疏注意力机制，同时通过使用与原始模型相同的数据分布确保公平比较。

#### 阶段一：持续预训练

第一阶段的重点是教会模型使用新的稀疏注意力架构。

**密集预热阶段：初始速成课程**

> **目标：** 通过让全新的**闪电索引器**模仿原始模型的注意力模式，教会它识别哪些 token 是“重要的”。

这是一个短暂但关键的初始阶段，仅持续 1,000 步（21 亿 token）。研究人员冻结了主模型并保持标准（密集）注意力激活，仅训练闪电索引器，要求其预测强大预训练主模型的注意力模式。
-   **方法：** 使用 **KL 散度损失** 衡量索引器预测与主模型注意力分数的接近程度。
-   **关键参数：**
    -   **学习率：** $10^{-3}$
    -   **批大小：** 16 个长度为 128K token 的序列。

**稀疏训练阶段：适应新现实**

> **目标：** 让整个模型适应由索引器选出的稀疏注意力模式。

这是主要训练阶段，持续 15,000 步（9437 亿 token）。研究人员在此阶段“开启”稀疏机制，解冻主模型，并联合训练所有组件。
-   **方法：** 模型现在被迫仅使用索引器选出的 **top-k**（$k=2048$）个 token 来预测下一个词。
-   **关键创新：** 通过将索引器从主计算图中分离，分别优化索引器和主模型，避免训练信号相互干扰。
    -   **主模型** 仅通过语言建模损失（预测下一个词）进行训练。
    -   **闪电索引器** 仅通过 KL 散度损失进行训练，以使其在选出的 $k$ 个 token 上与主模型注意力保持对齐。
-   **关键参数：**
    -   **学习率：** $7.3 \times 10^{-6}$
    -   **批大小：** 480 个长度为 128K token 的序列。

#### 阶段二：后训练——为公平比较而微调

为确保严格且公平的评估，后训练流程（包括算法和数据）与原始 DeepSeek-V3.1-Terminus 完全一致。

**专家蒸馏**

首先，团队为数学、竞赛编程和智能体编码等领域开发了专用模型。每个专家模型均从相同的 DeepSeek-V3.2 预训练检查点微调而来。通过大规模强化学习（RL），这些模型生成了高质量的领域特定数据，并用于“蒸馏”训练最终模型。

**混合 RL 训练**

最后，模型使用**分组相对策略优化（GRPO）** 进行微调。团队在此关键策略转变中，将推理、智能体和人类对齐训练合并为单一 RL 阶段。这种统一方法在不同技能间取得平衡，同时避免了多阶段训练中常见的“灾难性遗忘”问题。结果令人鼓舞：新稀疏模型的 RL 训练曲线与原始模型高度吻合，证明 DSA 是一个稳定且有效的改进。

---

*我们正在积极开展相关研究——[欢迎参与](https://github.com/Open-Superintelligence-Lab/deepseek-sparse-attention-research)*

### 研究问题

我们的实验旨在回答：

1. **稀疏注意力能否提升标准注意力架构的性能？**
2. **当应用于已高效的多头潜在注意力（MHLA）时，稀疏注意力是否能带来额外收益？**
3. **这些机制在不同序列长度下的扩展性如何？**

**训练时间有限**：每次实验仅训练 500-1000 步（在 1 块 Nvidia 4090 上约 5-10 分钟）。

### 实验一：标准注意力 vs 稀疏注意力

| 序列长度 | 标准损失 | 稀疏损失 | 改进幅度 | 标准准确率 | 稀疏准确率 |
|----------|----------|----------|----------|------------|------------|
| 64       | 8.52     | **3.56** | **提升 139%** | 4.3%       | **53.2%**  |
| 128      | 7.28     | **3.00** | **提升 143%** | 6.5%       | **57.6%**  |
| 256      | 7.15     | **1.78** | **提升 302%** | 7.6%       | **68.4%**  |

**关键发现**：我们的小型 LLM 在稀疏注意力下学习更快（研究进行中）

### 实验二：DeepSeek MHLA 密集 vs MHLA + 稀疏

| 序列长度 | MHLA 损失 | MHLA+稀疏损失 | 改进幅度 | MHLA 准确率 | MHLA+稀疏准确率 |
|----------|-----------|----------------|----------|-------------|-----------------|
| 64       | 7.43      | **6.64**       | **提升 12%** | 9.2%        | **15.5%**       |
| 128      | **6.85**  | 6.97           | 下降 2%    | 10.3%       | 10.3%           |
| 256      | 6.61      | **6.55**       | **提升 1%**  | 12.5%       | **13.2%**       |
| 1024     | **4.10**  | 6.91           | **下降 41%** | **32.2%**   | 10.7%           |
| 2048     | 6.64      | **6.63**       | **基本持平** | 11.9%       | **14.4%**       |

**关键发现**：结果喜忧参半——稀疏机制在短序列上有所助益，但在长序列上损害了 MHLA 性能。可能与实现方式有关（研究进行中）

### 速度分析

**实验一**：训练速度相近（两者均约 0.06 秒/步）  
**实验二**：稀疏版本因闪电索引器开销慢了 1-4%（按理说因处理 token 更少应更快🤔）

### 研究洞见

稀疏注意力或许并非仅仅是弱化版的密集注意力，它可能展现出独特优势，例如防止注意力稀释（attention dilution）。

### 未来研究方向（欢迎参与）：

## 核心架构
1. **为何索引器分数需要额外权重？**（$w_{t,j}^I$ 的必要性）
2. **不同序列长度下的最优 $k$ 值是多少？**

## 闪电索引器
3. **索引器性能如何随序列长度扩展？**
4. **扩展性如何影响索引器的准确性和计算效率？**

### 关于开放超智能实验室（Open Superintelligence Lab）

[开放超智能实验室](https://opensuperintelligencelab.com/) 致力于让全球任何人参与开源 AI 研究。我们开展此类实验以理解神经网络和大语言模型的基础机制，并分享研究成果。

我们的研究持续进行中，欢迎合作与反馈。这些实验代表正在进行的研究，可能存在缺陷或局限，我们鼓励独立验证我们的发现。

---

## 未来研究方向

![MLA 框架中的 MHA 与 MQA 模式](/content/deepseek-sparse-attention/MHA-and-MQA-modes-of-MLA.png)

上图展示了 MLA 框架中的多头注意力（MHA）和多查询注意力（MQA）模式。

---

*本研究是我们对高效注意力机制持续探索的一部分。结果为初步结论，随着更多实验的开展可能会修正。*