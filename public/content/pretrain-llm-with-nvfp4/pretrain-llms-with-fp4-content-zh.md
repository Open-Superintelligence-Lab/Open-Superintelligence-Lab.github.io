---
hero:
title: "使用 NVFP4 预训练大语言模型"
subtitle: "⚡ NVFP4：训练速度提升 2-3 倍，内存占用减少 50%"
tags:
- "⏱️ 技术深度解析"
- "📄 研究论文"
---

[研究论文](https://arxiv.org/pdf/2509.25149) • [实现 PR](https://github.com/NVIDIA/TransformerEngine/pull/2177)

# 使用 NVFP4 预训练大语言模型的技术指南

*本文基于技术报告《Pretraining Large Language Models with NVFP4》，概述 NVIDIA 的 4 位浮点格式 NVFP4，以及如何高效、准确地将其应用于大语言模型（LLM）预训练。*

随着大语言模型（LLM）规模的不断扩大，更高效的训练方法变得至关重要。尽管 8 位浮点（FP8）训练已被广泛采用，但 4 位浮点（FP4）格式可在计算速度和内存使用方面带来进一步的提升。本指南对 NVIDIA 提出的 **NVFP4** 4 位格式及其在 LLM 预训练中的成功实现方法进行技术性总结。

**架构说明**：本指南基于 **Mamba-Transformer** 架构的实验，该架构结合了 Mamba 状态空间模型与 Transformer 组件。

## 背景：数值精度的关键概念

在深入探讨 NVFP4 之前，有必要了解一些基础概念。

您可以将以下内容复制到 AI 聊天机器人中，以获取个性化教学。

- **数值精度**：在深度学习中，数字通常以浮点格式存储（例如 FP32、FP16、BF16、FP8、FP4）。格式名称中的数字表示用于表示单个数值的比特数。更多比特（如 FP32）可表示更宽的数值范围和更高精度（更多细节），但会消耗更多内存且计算速度更慢。更少比特（如 FP4）则更快、更节省内存，但精度较低。

- **量化**：这是将张量从高精度格式（如 FP32）转换为低精度格式（如 FP4）的过程。这是加速模型训练和推理的核心技术。然而，该过程可能导致信息丢失，若处理不当，会降低模型的准确性。

- **动态范围**：指一种数值格式所能表示的数值范围，从最小非零值到最大值。量化时，我们通常会对张量中的值进行缩放，使其适配目标格式（如 FP4）有限的动态范围。一个关键挑战是：单个极大值（“异常值”）可能主导整个范围，迫使所有其他较小值被量化为零或接近零，从而有效抹除其信息。

## 缩放中的异常值问题（示例）

异常值的存在——例如 `[0.5, -0.2, 1.1, -0.8, 50.0]` 中的 `50`——是量化的主要挑战。因为一个数值块的缩放因子由其中最大值决定，一个异常值就可能破坏该块中所有其他数值的精度。

- **场景**：假设我们有一个小数值块：`[0.5, -0.2, 1.1, -0.8, 50.0]`
- **异常值**：`50.0` 是一个显著的异常值。
- **缩放**：要将此块量化为 FP4（其最大可表示值为 `6.0`），我们必须使用相同的缩放因子将所有数值缩小。`缩放因子 = 6.0 / 50.0 = 0.12`（缩放因子基于最大绝对值计算）。
- **结果**：缩放（乘法）后，该块变为 `[0.06, -0.024, 0.132, -0.096, 6.0]`。  
  此时，FP4（E2M1 格式）可表示的值为 ±0、±0.5、±1、±1.5、±2、±3、±4 和 ±6。这意味着缩放后的任何值都将被舍入到这些离散值中最接近的一个。因此，可表示范围为 -6 到 +6，且仅有这些特定值可用。
- **信息丢失**：当这些新值被转换为最接近的 FP4 可表示值（如 `±0`、`±0.5`、`±1.0`……）时，前四个值因过小而很可能全部被舍入为零。它们原本包含的信息就此丢失，只有异常值保留了其重要性。NVFP4 的技术正是为缓解此类问题而设计。

## NVFP4 的技术优势

从 FP8 过渡到 FP4 可带来 **2-3 倍的算术性能提升**——主要是通用矩阵乘法（GEMM）操作的吞吐量提升（GEMM 是 Transformer 的计算核心）——以及 **50% 的内存占用减少**。然而，更低的精度也带来了挑战。NVFP4 通过以下关键特性应对这些问题：

- **两级缩放实现高精度表示**：简而言之，NVFP4 使用两个缩放因子：一个应用于整个张量（如权重或激活值，通常包含数百万个数值），另一个应用于张量内每个 16 元素的小块。

![NVFP4 矩阵存储格式](/content/pretrain-llm-with-nvfp4/images/NVFP4_matrix_storage_format.png)  
*图 1：以 NVFP4 格式存储的 16x32 矩阵。每个块包含 16 个连续的 FP4 元素（灰色和绿色），以及一个 FP8 缩放因子（黄色）。每个块中绝对值最大的元素（绿色）被缩放至 FP4 的最大可表示值，并可通过块缩放因子恢复。此外还有一个每张量的 FP32 缩放因子（图中未显示）。*

NVFP4 使用两个不同的缩放因子，这是其最关键特性。为理解这一点，我们先定义两个术语：

- **张量（Tensor）**：指包含大量数值的多维数组，用于存储模型的权重或激活值。这些是神经网络中的核心数据结构，规模可能极其庞大。以下是论文中描述模型的具体示例：
  - **权重张量**：在 12B 模型中，单个 FFN 权重张量可包含超过 1.04 亿个数值（形状：[5120, 20480]）。
  - **激活张量**：在 8B 模型中，层间激活值（层输出）可形成形状为 [6.3M, 4096] 的二维张量（来自批量大小 768 × 序列长度 8192 × 模型维度 4096）。

- **块（Block）**：指张量中一个固定大小的小块。在 NVFP4 中，块是 16 个连续数值的组合。因此，上述庞大的权重和激活张量将被划分为成千上万个这样的小块进行量化。

两级缩放的工作方式如下：

1. **粗粒度、每张量缩放（FP32）**：首先，基于整个张量的绝对最大值（max(abs(tensor))）计算一个缩放因子。该因子以高精度 **FP32** 存储，对张量中所有数值进行粗略的全局调整，使其进入更易处理的中间范围，同时避免缩放因子本身成为误差源。使用高精度格式 FP32 至关重要，因为不精确的缩放因子会错误地缩小张量中的每个值，在最终量化步骤之上叠加额外误差。

2. **细粒度、每块缩放（FP8）**：在应用第一次缩放后，张量被划分为成千上万个 16 元素的小块。对**每个**小块，计算第二个更精确的缩放因子。该局部因子以 **FP8** 存储，进行精细调整，将 16 个值完美映射到 FP4 极其有限的范围内。此处使用的 FP4 格式（E2M1）仅能表示 ±0、±0.5、±1、±1.5、±2、±3、±4 和 ±6。

这种双重方法之所以强大，是因为它允许高度局部化的自适应。张量某部分的一个大异常值仅会影响其所在 16 元素小块的缩放，完全不影响其他所有块的量化。相比单一缩放因子，这显著保留了更多信息。

- **更小的块尺寸以提升动态范围**：NVFP4 使用 16 元素的微型块尺寸。这对**捕捉局部动态范围**至关重要。简而言之，如果一个数值块包含一个大异常值，在较小块（如 16 元素）中，仅其他 15 个数值受影响；而在较大块（如 32 元素）中，同一异常值会迫使所有其他 31 个数值使用精度更低的缩放，可能导致更多信息丢失。更小的块尺寸可隔离异常值的影响。

- **原生硬件支持**：NVIDIA Blackwell GPU 架构的 Tensor Core 原生支持 NVFP4，为 GEMM 操作提供显著的硬件加速。

![NVIDIA Blackwell Tensor Core](/content/pretrain-llm-with-nvfp4/images/NVIDIA_Blackwell_Tensor_Cores.png)  
*表 1：NVIDIA Blackwell Tensor Core。该表展示了 NVFP4 及其他格式在 GB200 和 GB300 GPU 上相比 BF16 的加速比。*

这些设计选择使 NVFP4 在提供 4 位精度效率的同时，**缓解了典型权衡问题**，即激进量化通常导致的模型精度损失和训练不稳定性。

## NVFP4 训练的核心方法

要获得与 FP8 相当的训练效果，需要一套特定技术。以下是推荐用于 NVFP4 稳定、准确预训练的方法。

![NVFP4 训练技术组合](/content/pretrain-llm-with-nvfp4/images/combining_NVFP4_training_techniques.png)  
*图 8：NVFP4 训练技术组合：最后四个块中的线性层使用 BF16、二维权重缩放、权重梯度上的随机哈达玛变换（RHT）以及梯度上的随机舍入。图为在 1T token 上训练的 1.2B 模型的验证损失相对差异。*

### 1. 混合精度策略

将整个模型量化为 FP4 可能导致发散（模型停止学习）。混合精度策略对稳定性至关重要。

![NVFP4 量化线性层计算流程](/content/pretrain-llm-with-nvfp4/images/NVFP4_quantized_linear_layer_compute_flow.png)  
*图 5：NVFP4 量化线性层的计算流程示意图。所有 GEMM 操作均将其输入量化为 NVFP4。*（理解此图需深入分析并详细理解论文）

**实现方式**：
- 在线性（全连接）层的大部分 GEMM 操作中使用 NVFP4。
- 将少量数值敏感的线性层（约 15%）保持在更高精度格式（如 BF16）。论文发现，网络的**最后几层**最为敏感，因其需要比 FP4 更大的动态范围和更高精度。保持模型开头和结尾少数几个块的高精度通常足以确保训练稳定。

![NVFP4 与 BF16 精度切换](/content/pretrain-llm-with-nvfp4/images/NVFP4_BF16_precision_switching.png)  
*图 7：训练后期切换至更高精度。图为在 10T token 上训练的 12B 模型的验证损失相对差异。NVFP4 在整个训练期间（绿色）均使用第 4 节所述方法。前向和反向传递中的张量（蓝色）、仅前向传递中的张量（橙色）以及仅反向传递中的张量（紫色）在 8.2T token 时从 NVFP4 切换为 BF16，直至训练结束。另有一组在约 10T token 时切换精度的实验（红色）。在反向传递中切换精度时使用一维权重缩放，因为在此设置下其效果略优于二维权重缩放。*

- 将其他关键组件保持在其原始精度（BF16 或 FP32）以确保数值稳定性。这包括嵌入层、输出投影头、归一化层、非线性激活函数以及注意力机制的大部分组件（如 softmax 等）。仅 Transformer 块中的大型 GEMM 操作被量化为 FP4。

![线性层对量化的敏感性](/content/pretrain-llm-with-nvfp4/images/linear_layer_sensitivity_to_quantization.png)  
*图 9：线性层对量化的敏感性。除模型开头和结尾少数几个块外，所有线性层均使用 NVFP4。图为在 1T token 上训练的 1.2B 模型的验证损失。*

### 2. 随机哈达玛变换（RHT）用于异常值管理

这是一个巧妙的技巧。若要将 `激活值 × 权重` 均量化为 FP4，可使用一个矩阵 `H`，满足 `H × H = I（单位矩阵）` → `H` 是正交矩阵。

然后可进行 `(激活值 × H) × (H × 权重)`。

结果与 `激活值 × 权重` 相同，但在量化 `激活值` 和 `权重` 时可减少异常值问题（精度损失）。

**实际示例**：
- **原始块**：考虑一个包含四个数值的小块：`[1.0, -2.0, 1.5, 30.0]`。
- **问题**：异常值为 `30.0`。量化时，所有数值必须基于此大值进行缩放，导致前三个数值精度丢失。
- **RHT 变换**：应用哈达玛变换后，该块变为：`[15.25, -12.75, -16.25, 15.75]`。
- **结果**：大异常值 `30.0` 消失。能量被重新分配，新最大绝对值仅为 `16.25`。
- **优势**：当此新块缩放至 FP4 范围时，缩放因子几乎大了一倍。这意味着其他数值被缩放为更大、更易区分的值，在最终舍入至 FP4 前保留了更多信息。

**示例背后的数学原理**：

该变换是矩阵-向量乘法。对于示例中的四数值块，计算使用归一化的 4x4 哈达玛矩阵（`H`）：
```
        [ 0.5,  0.5,  0.5,  0.5 ]
H =     [ 0.5, -0.5,  0.5, -0.5 ]
        [ 0.5,  0.5, -0.5, -0.5 ]
        [ 0.5, -0.5, -0.5,  0.5 ]
```
将原始块 `[1.0, -2.0, 1.5, 30.0]` 与此矩阵相乘（`[1.0, -2.0, 1.5, 30.0] × H`）即得新值。

**这是什么矩阵？** 矩阵 `H` 是归一化的**哈达玛矩阵**，一种因其关键特性——**正交性**而选定的固定常数矩阵。该特性保证变换完全可逆（`H × H^T = I`）。对研究人员而言，这意味着它不是学习参数，而是一种标准数学工具，可将数据临时无损地变换为更利于量化的格式。

**实现方式**：

- **针对正确操作**：RHT 并非处处应用。论文发现，其对**权重梯度（`Wgrad`）计算**的稳定性最为关键。这是反向传播中计算权重更新的部分。在其他地方（如前向传播）应用 RHT 无益，甚至可能损害性能。

![哈达玛变换对验证损失的影响](/content/pretrain-llm-with-nvfp4/images/hadamard_transform_impact_on_validation_loss.png)  
*图 11：在不同 GEMM（前向传播 Fprop、数据梯度 Dgrad 和权重梯度 Wgrad）中应用随机哈达玛变换（RHT）对训练的影响，与无 RHT 对比。RHT 实验中，所有变换在整个训练中使用固定随机种子。除最后四个块外，所有线性层均使用 NVFP4 量化。图为在 1T token 上训练的 1.2B 参数模型相对于 BF16 基线的验证损失相对变化。*

- **选择有效矩阵尺寸**：变换通过将数据与“哈达玛矩阵”相乘实现。更大矩阵能更有效地分散异常值，但计算开销更大。论文发现，**16x16 矩阵**在大模型中提供了最佳权衡，在强异常值抑制与计算开销之间取得平衡。

![哈达玛矩阵尺寸效应](/content/pretrain-llm-with-nvfp4/images/hadamard_matrix_size_effect.png)  
*图 12：不同哈达玛矩阵尺寸的影响。在 12B 模型的前 3.4T token 训练中，Wgrad 张量使用 16×16 变换，之后切换为 4×4 或 128×128 直至训练结束（共 4T token）。图为 NVFP4 训练损失相对于 FP8 基线的相对差异。NVFP4 训练使用第 4 节指定的方法。*

- **使用随机化解决“结构对齐”问题**：RHT 中的“随机”是对一种罕见但关键失败情况的简单修复。该问题称为**结构对齐**，当数据块的符号模式偶然与固定哈达玛矩阵中的模式完全匹配时发生。这种对齐会导致变换失败，反而**产生**新异常值。
  - **问题实例**：假设数据块为 `[10, 8, -12, -9]`，其符号模式为 `[+, +, -, -]`。哈达玛矩阵中某行具有相同的 `[+, +, -, -]` 模式。应用变换时，匹配符号导致所有数值同向叠加（`10 + 8 + 12 + 9 = 39`），产生新的巨大异常值。
  - **修复实例**：随机化通过随机翻转变换行的符号来解决此问题，将模式变为如 `[+, -, +, -]`。当此新（不对齐）模式应用于相同数据时，数值相互抵消（`10 - 8 - 12 + 9 = -1`），避免产生新异常值。
  - **实践要点**：为此，哈达玛矩阵本身需随机化。论文发现，在整个训练中创建一个随机矩阵并重复使用即可。

![哈达玛变换随机化效应](/content/pretrain-llm-with-nvfp4/images/hadamard_transform_randomization_effect.png)  
*图 13：哈达玛变换随机化效应。前 3.4T token 使用单一固定种子，之后切换为以下随机化选项之一：所有层使用单一固定种子、每个变换使用唯一种子、或不使用随机符号向量。图为在 4T token 上训练的 12B 模型相对于 FP8 基线的训练损失相对差异。NVFP4 训练使用第 4 节指定的方法。*

### 3. 二维（2D）权重缩放实现一致量化

为理解此技术，想象一个来自大矩阵的 2x2 权重小块：`[[W₁₁, W₁₂], [W₂₁, W₂₂]]`。

**问题（不一致的一维缩放）**：  
训练中，此块在两个主要阶段处理方式不同：
- **前向传播（按行）**：权重 `W₁₁` 与其同行 `W₁₂` 分组。它们基于 `max(abs(W₁₁), abs(W₁₂))` 共同缩放。
- **反向传播（按列）**：因反向传播中权重矩阵转置，`W₁₁` 现与其同列 `W₂₁` 分组。它们基于 `max(abs(W₁₁), abs(W₂₁))` 共同缩放。  
例如，若 `W₁₁ = 2.0`、`W₁₂ = 10.0`、`W₂₁ = 0.5`，则前向传播（按行缩放）中 `W₁₁` 使用其行最大值（`max(2.0, 10.0) = 10.0`）量化，而反向传播（按列缩放）中使用其列最大值（`max(2.0, 0.5) = 2.0`）量化。因此 `W₁₁` 获得两个不同量化值，破坏链式法则。

**解决方案（一致的二维缩放）**：  
不按行缩放，而是将整个 2x2 块视为单一单元。
- **工作原理**：为整个块计算**单一**缩放因子，基于所有四个权重的最大绝对值：`max(abs(W₁₁), abs(W₁₂), abs(W₂₁), abs(W₂₂))`。
- **结果**：因整个方块使用相同缩放因子，无论按行或按列处理，`W₁₁` 的量化值在前向和反向传播中均保持一致。

**实践要点**：论文将此原则应用于更大的 16x16 块。对权重张量使用 16x16 二维块缩放以确保一致性。对激活值和梯度，标准一维缩放已足够，因训练对这些张量的不一致性较不敏感。

![张量一致性效应](/content/pretrain-llm-with-nvfp4/images/tensor_consistency_effect.png)  
*图 14：张量一致性效应。图为在 1T token 上训练的 1.2B 模型相对于 BF16 基线的验证损失相对差异。NVFP4 应用于权重或激活值。不同缩放因子选择：沿相同维度的 1×16 块缩放、沿不同维度的 1×16 块缩放、16×16 块缩放，以及全局 FP32 每张量缩放。*

> **为何权重比激活值更敏感？**  
> 核心区别在于它们在训练中的角色和生命周期：
> - **权重是模型的“长期记忆”**。它们是持久参数，在整个训练过程中学习和更新。权重值的不一致性会产生持久、级联的影响，因其在每次前向和反向传播中均被使用，随时间推移会破坏学习信号。
> - **激活值是“短暂思绪”**。它们是瞬时值，在前向传播中计算，仅在对应反向传播中使用一次，随后丢弃。此处的不一致性影响更局部且短暂。  
> 因此，对持久权重张量采用二维缩放是关键投入，但对瞬时激活张量则收益递减。

### 4. 随机舍入实现无偏梯度

量化时，许多值会落在 FP4 有限可表示点之间。标准舍入（如始终舍入至最近值）可能引入系统性偏差。例如，若略多数值向下舍入，则会产生持续的向下漂移，损害学习。

**问题：标准舍入的系统性偏差**

假设 FP4 仅可表示 `0` 和 `1`。现考虑四个高精度梯度值：`[0.6, 0.7, 0.8, 0.9]`。
- **标准舍入**：使用“舍入至最近”规则，所有四个值均向上舍入为 `1`。新块为 `[1, 1, 1, 1]`。
- **偏差**：原始数值平均值为 `0.75`，舍入后平均值为 `1.0`。舍入系统性地推高了平均值，向梯度信号引入向上偏差。数千训练步后，此微小但持续的误差可能使模型偏离正轨。

**解决方案：无偏随机舍入**

随机舍入是一种概率方法，可消除此类平均偏差。
- **工作原理**：不进行确定性舍入，而是以与数值到两个最近可表示值距离成比例的概率向上或向下舍入。
- **实际应用**：对值 `0.7`，有 70% 概率向上舍入为 `1`，30% 概率向下舍入为 `0`。
- **结果**：在大量数值上，此方法统计无偏。舍入 `0.7` 的*期望值*为 `(0.7 × 1) + (0.3 × 0) = 0.7`，与原始值完全相同。它为每个单独操作引入少量随机性（噪声），但确保整体梯度信号长期保持准确。

**实践要点**：论文发现，对梯度张量量化时**必须使用随机舍入**。但对前向传播中的权重和激活值，标准**舍入至最近偶数**更佳，因随机舍入的噪声可能有害。

![12B 模型消融实验](/content/pretrain-llm-with-nvfp4/images/ablations_on_12B_model.png)  
*图 4：12B 模型在 10T token 上的消融实验。消融研究从使用 NVFP4（除前两个和后八个块外）训练至 3.43T token 的模型开始，每次移除一个方法组件：随机舍入（SR）、随机哈达玛变换（RHT）、二维缩放（2D）和更少 BF16 块。相对差异定义为 (FP8 - 实验) / FP8，负值表示实验效果更差。*

![不同张量上的随机舍入](/content/pretrain-llm-with-nvfp4/images/stochastic_rounding_on_different_tensors.png)  
*图 10：随机舍入应用于不同张量：梯度、激活值、权重和反向传播张量。除最后四个块外，所有线性层均使用 NVFP4。图为在 1T token 上训练的 1.2B 模型的验证损失。*

## 实证验证：12B 模型在 10T token 上的训练

论文通过在 10 万亿 token 上预训练 120 亿参数模型，验证了上述四部分方法。

**结果**：
- **训练损失**：NVFP4 训练模型的验证损失在整个 10T token 训练中与 FP8 基线高度吻合。

![NVFP4 与 FP8 对比](/content/pretrain-llm-with-nvfp4/images/NVFP4_vs_FP8.png)  
*图 2：12B 模型在 10T token 上 NVFP4 与 FP8 预训练的验证损失。*

- **下游任务准确性**：NVFP4 模型在包括推理、数学和代码生成在内的多样化下游任务中，准确率与 FP8 基线相当。例如，NVFP4 模型在 MMLU-pro 上的准确率为 62.58%，与 FP8 模型的 62.62% 几乎相同。

![NVFP4 与 FP8 任务准确率](/content/pretrain-llm-with-nvfp4/images/task_accuracy_nvfp4_vs_fp8.png)  
*图 3：NVFP4 与 FP8 在 10T token 预训练过程中的任务准确率对比。*

此结果构成了迄今公开记录中最长的 4 位训练实验，证明了 NVFP4 在大规模预训练中的可行性。

## 格式对比：NVFP4 与 MXFP4

在 8B 参数模型的直接对比中，NVFP4 的收敛性优于 MXFP4 格式。

![NVFP4 与 MXFP4 对比](/content/pretrain-llm-with-nvfp4/images/NVFP4_vs_MXFP4_comparisons.png)  
*图 6：NVFP4 与 MXFP4 对比：(a) 训练损失差异；(b) 不同 token 预算下的验证困惑度。*

为达到与 NVFP4 训练模型相同的最终训练损失，MXFP4 模型需要**多 36% 的训练 token**。这表明 NVFP4 的设计带来了更高的样本效率。

## 结论

结合指定训练方法的 NVFP4 能够实现大语言模型在 4 位精度下的稳定、准确预训练。该方法在计算吞吐量和内存使用方面提供显著效率增益，同时不损害模型性能。NVIDIA Transformer Engine 已全面支持 NVFP4。

---

***来源***：*本指南是对技术报告《[Pretraining Large Language Models with NVFP4](https://arxiv.org/pdf/2509.25149v1)》的总结。完整细节请参阅原始出版物。*