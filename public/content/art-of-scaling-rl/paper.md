
19
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
Under review as a conference paper at ICLR 2026
OpenAI. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.
OpenAI. Introducing OpenAI o3 and o4-mini. https://openai.com/index/
introducing-o3-and-o4-mini/, 2025. Accessed: 22 September 2025.
David Owen. How predictable is language model benchmark performance? arXiv preprint
arXiv:2401.04757, 2024.
Alex Piche, Rafael Pardinas, Ehsan Kamalloo, and Dzmitry Bahdanau. Pipelinerl. 2025. URL
https://huggingface.co/blog/ServiceNow/pipelinerl.
Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig Schmidt, and Yair Carmon. Resolving
discrepancies in compute-optimal scaling of language models, 2025. URL https://arxiv.
org/abs/2406.19146.
Abhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep
Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv
preprint arXiv:2506.10910, 2025.
Yangjun Ruan, Chris J. Maddison, and Tatsunori Hashimoto. Observational scaling laws and the pre-
dictability of language model performance, 2024. URL https://arxiv.org/abs/2405.
10938.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347.
ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang,
Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb rea-
soning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathemati-
cal reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the
imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint
arXiv:2206.04615, 2022.
xAI Team. Grok 4. 2025. URL https://x.ai/news/grok-4.
Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P Lillicrap, Kenji Kawaguchi,
and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning.
arXiv preprint arXiv:2405.00451, 2024.
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang
Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu,
Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin
Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang,
Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui
Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang
Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger
Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan
Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388.
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian
Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system
at scale. arXiv preprint arXiv:2503.14476, 2025.
Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. What’s behind ppo’s collapse in
long-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025.
12
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
Under review as a conference paper at ICLR 2026
Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi
Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for
advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.
Ruiqi Zhang, Daman Arora, Song Mei, and Andrea Zanette. Speed-rl: Faster training of reason-
ing models via online curriculum learning, 2025. URL https://arxiv.org/abs/2506.
09016.
Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang,
Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy op-
timization, 2025a. URL https://arxiv.org/abs/2507.18071.
Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang,
Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint
arXiv:2507.18071, 2025b.
Haizhong Zheng, Yang Zhou, Brian R. Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and
Beidi Chen. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective
rollouts, 2025c. URL https://arxiv.org/abs/2506.02177.
A APPENDIX
A.1 RELATED WORK
A wave of recent work has applied Reinforcement Learning (RL) to improve the reasoning abili-
ties of large language models (LLMs); often achieving state-of-the-art results on challenging tasks
(OpenAI, 2024; Guo et al., 2025; Seed et al., 2025). OpenAI’s o1 series of models established that
large-scale RL can substantially enhance long-horizon reasoning, but did not release any details on
how these models were trained. Deepseek R1 (and R1-Zero) (Guo et al., 2025) provided the first
comprehensive study on training high-performing and long Chain-of-Thought (CoT) models primar-
ily via RL, documenting emergent behaviours under extended RL without any reliance on reward
models (Lightman et al., 2023) or Monte Carlo Tree Search (MCTS) (Xie et al., 2024).
The earliest widely referenced RLVR (verifiable-reward) algorithm underlying this wave of reason-
ing development is Group Relative Policy Optimization (GRPO), introduced in Shao et al. (2024).
GRPO is a critic-free, group-relative policy gradient with PPO-style clipping that replaces a learned
value baseline with group baselines to reduce computational cost and stabilize credit assignment
for long CoTs. While GRPO catalyzed rapid progress, subsequent work document its limitations
(token-level clipping, model collapse risks) and motivate different group- or sequence- level vari-
ants (Yu et al., 2025; Yue et al., 2025; Hu et al., 2025b; Zheng et al., 2025b).
Yu et al. (2025) propose the Decoupled clip and Dynamic Sampling Policy Optimization (DAPO),
where they decouple εlow and εhigh clipping in the GRPO objective and do Clip-Higher for εhigh
to avoid entropy collapse. Furthermore, they do dynamic sampling of prompts in a given batch to
avoid samples with zero variance (or advantage) which contribute zero policy gradients. Finally,
they employ token-level loss aggregation unlike GRPO, which uses sample-level loss averaging.
With these modifications, they are able to surpass the vanilla GRPO baseline while avoiding entropy
collapse in the RL training. In parallel, Yue et al. (2025) develop VAPO; a value-augmented PPO
tailored for long CoTs with strong stability and outperforming value-free baselines like GRPO and
DAPO. They combine value pre-training and decoupled Generalized Advantage Estimation (GAE)
from VC-PPO (Yuan et al., 2025), loss objective modifications from DAPO, and propose length-
adaptive GAE to come up with an open recipe, VAPO, that has been used to train large MoE models
in Seed et al. (2025). Similarly, other technical report like Magistral (Rastogi et al., 2025), Kimi-
k1.5 (Kimi Team et al., 2025b), Minimax-01 (Li et al., 2025a) detail various details on their RL
training recipes, but don’t share extensive experiments on why their design choices are better than
the baselines.
A.2 BACKGROUND
Group Relative Policy Optimization (GRPO) GRPO (Shao et al., 2024) adapts PPO Schulman
et al. (2017) for LLM fine-tuning with verifiable rewards. For a given prompt x, the old policy
13
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
Under review as a conference paper at ICLR 2026
πgen(θold) generates G candidate completions {yi}Gi=1, each assigned a scalar reward ri. To empha-
size relative quality within the group, rewards are normalized as
ˆAi = ri −mean({rj}Gj=1)
std({rj}Gj=1) + ε . (4)
Each completion yi of length |yi|contributes at the token level through ratios
ρi,t(θ) = πtrain(yi,t |x,yi,<t,θ)
πgen(yi,t |x,yi,<t,θold). (5)
The GRPO objective averages across both completions and tokens:
JGRPO(θ) = Ex∼D,
{yi}G
i=1∼πgen(·|x,θold )

 1
G
G∑
i=1
1
|yi|
|yi|∑
t=1
min
(
ρi,t(θ) ˆAi, clip(ρi,t(θ),1 ±ε) ˆAi
)
 (6)
Thus GRPO preserves token-level policy ratios as in PPO, while using sequence-level, group-
normalized advantages to stabilize learning under sparse rewards.
Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) DAPO (Yu et al., 2025)
extends GRPO with two key modifications. First, it replaces symmetric clipping with asymmet-
ric clipping, using distinct thresholds for upward and downward deviations: clipasym(ρ,a) =
clip(ρ, 1 −ε−,1 + ε+), where ε− and ε+ are hyper-parameters.
Second, DAPO changes the aggregation scheme to operate at the prompt level. For a given prompt
x ∼ D, the old policy produces G completions {yi}Gi=1 with advantages {ˆAi}(Equation (4)). Let
T = ∑G
i=1 |yi|denote the total number of tokens across all completions. With token-level ratios as
in Equation (5). The DAPO surrogate objective is
JDAPO(θ) = Ex∼D,
{yi}G
i=1∼πgen(·|x,θold )

1
T
G∑
i=1
|yi|∑
t=1
min
(
ρi,t(θ) ˆAi, clipasym(ρi,t(θ), ˆAi) ˆAi
)
 (7)
This prompt-level normalization ensures that each token contributes equally to the prompt’s loss,
regardless of the number or length of its sampled completions. DAPO also introduces dynamically
dropping 0-variance prompts from the batch during training and filling the batch with more prompts
until the batch is full. We skip that change here since its effect is similar to having a larger batch
size.
A.3 TRAINING SETUP
Datasets For the small-scale SFT, we use a curated datamix of reasoning traces. We filter this
dataset by removing trivial prompts, discarding solution traces exceeding 12k tokens, and decontam-
inating with AIME 2024/2025 (AoPS, 2025) and MATH-500 (Hendrycks et al., 2021) benchmarks.
For the RL stage, we use the Polaris-53K dataset (An et al., 2025) for most of our runs; additionally
using the Deepcoder dataset (Luo et al., 2025) for runs with both math and code.
Supervised Fine-tuning We run SFT using a batch size of 2M tokens, max sequence length of
32768, and a learning rate of 3 ×10−5 using the AdamW optimizer (Loshchilov & Hutter, 2019) on
32 H100 GPU nodes for approximately 4 epochs and 32B tokens in total.
Reinforcement Learning We allocate 14k generation budget during RL training, where 12k to-
kens are allocated to the intermediate reasoning (“thinking”), followed by 2k tokens for the final
solution and answer. We sample 48 prompts in each batch, each with 16 generations per prompt.
Thus, we get the total batch size as 768 completions per gradient update step. The rewards are given
as ±1 to correct and incorrect traces respectively.
We use automated checkers like Sympy (Meurer et al., 2017) or Math-Verify1 for assessing
the correctness of the final answer for math problems after stripping out the thinking trace
1 https://github.com/huggingface/Math-Verify
14
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
Under review as a conference paper at ICLR 2026
(a) (b)
(c) (d)
Figure 7: Comparing (a) loss aggregation, (b) different advantage normalization techniques, (c) “zero” vari-
ance filtering, and (d) adaptive prompt sampling.
(<think>···</think>). We use a custom code execution environment for coding problems
involving unit tests and desired outputs.
We use a constant learning rate of 5 ×10−7, AdamW optimizer (Loshchilov & Hutter, 2019) with
ε = 10−15, weight decay of 0.01, and a linear warmup of 100 steps. We use 80 Nvidia GB200
GPUs for a single run, with a compute budget ranging from 3.5-4K GPU hours for ablating different
design choices, 16K for the leave-one-out experiments, and finally 30k-100K GPU hours for our
larger scale runs.
A.4 FORWARD ABLATIONS
We show additional results for Section 3.2 in Figures 7a-7d.
A.5 LEAVE ONE OUT (LOO) ABLATIONS
We plot the remaining leave one out plots in Figure 8
A.6 COMPARING ALGORITHMS
Consistent with observations in large-scale pre-training, where the loss exhibits a sharp initial drop
before settling into a predictable power-law decay (Li et al., 2025b), we observe a similar two-phase
behavior in RL. The mean reward increases rapidly, almost linearly, during the first ∼epoch (∼1.1k
steps), after which the curve follows power-law behavior. Our sigmoidal-law fits are applied to this
latter portion of the training curve.
15
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
Under review as a conference paper at ICLR 2026
(a) (b) (c)
Figure 8: Comparison of different leave-one-out strategies using 16k GPU-hours compute budget.
Unlike pre-training, our goal is not to predict the performance of a fixed recipe, but to identify
which algorithms and design choices scale reliably. Achieving highly robust fits typically requires
very large runs with hundreds or thousands of evaluation points, which is impractical in our setting
for two reasons. First, running all ablations at such scale would be computationally prohibitive.
Second, many RL algorithms we compare are themselves not scalable to such extreme budgets: they
often saturate much earlier or even degrade with more compute due to instability. For example, our
baseline method (Section 3.2) destabilizes beyond ∼ 3500 GPU-hours, since overlong generation
truncations exceed 10% of generations - reducing the effective batch size. See A.15 for the extended
training curve.
As we ablate across different axes in Section 3.2, we discover design choices that improve stability
at higher compute. Some ablated variants can scale further, e.g., ∼ 5k GPU hours for ε = 0.26 in
DAPO, ∼6k GPU hours with the FP32 precision fix (Section 3.2), and ∼7k GPU hours for CISPO.
Once we combine the best design choices, we obtain a stable and scalable recipe, which allows us
to run leave-one-out (LOO) experiments for ∼1200 GPU hours per run.
A.7 FITTING CURVES
We fit the power-law equation in Equation (1) to the mean reward on our held-out evaluation set.
This set consists of 1k prompts held out from the POLARIS (An et al., 2025) math dataset, with 16
generations sampled every evaluation step performed at 100 steps intervals.
Directly fitting all three parameters {a,b,c}is challenging. Instead, we perform a grid search over
c ∈ {0.45,0.46,...,0.8}, and for each candidate c fit a and b. The best fit (measured by sum of
squared residuals) across this grid is selected as the final curve. We use SciPy’s curve fit with
default initialization; varying initialization strategies produced identical results.
To estimate the error margin of our fits, we trained three independent runs of ScaleRL with batch
size 768 and generation length 14k (as used in Section 3.3). For these runs, we refined the grid
search with c increments of 0.005 (i.e., c ∈ {0.455,0.460,0.465,...,0.800}). We found that the
fitted c values varied by at most ±0.02, suggesting this as a reasonable error margin on asymptotic
performance estimates. Estimating the error margin for fitted b value is tough, as different algrithms
with different c values can have different error margin for b. However, for the purpose of comparing
algorithms, we can safely deduce that if two methods achieve similar c values (within 0.02), the one
with higher b is at least as good in terms of scalability (Section A.6).
A.8 CONTROLLING GENERATION LENGTH
One common concern in reasoning RL is to control exploding generation lengths, which harms both
training efficiency and stability (Appendix A.15). We consider two approaches: (a) interruptions,
used in works like GLM-4.1V (GLM-V Team et al., 2025), and Qwen3 (Yang et al., 2025) and
(b) length penalties, used in works like DAPO (Yu et al., 2025), Kimi (Kimi Team et al., 2025b),
Magistral (Rastogi et al., 2025), and Minimax-M1 (MiniMax et al., 2025).
Interruptions forcibly stop generation by appending a marker phrase such as “Okay, time is up.
Let me stop thinking and formulate a final answer </think>”, signaling the model to terminate its
16
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
Under review as a conference paper at ICLR 2026
reasoning and produce a final answer. In our setup, the interruptions tokens are placed randomly in
between [10k,12k] token length, to induce generalization to different generation lengths.
Length penalties instead reshape the reward. Following DAPO (Yu et al., 2025), we penalize overly
long completions with a tolerance interval Lcache:
Rlength(y) = clip
(Lmax −|y|
Lcache
−1,−1,0
)
(8)
This penalty is added only to the correct traces, discouraging excessively long generations. In the
length penalty experiment, we set Lmax = 14k tokens and Lcache = 2k tokens.
In Section 3.3, we compare length penalty and interruption at a scale of 12k GPU-Hours. We
find that replacing interruption with length penalty in our final ScaleRL recipe does not improve
performance.
A.9 PIPELINERL
Using the baseline setup, we ablated the off-policy parameter in PipelineRL (Figure 9a). Both 4 and
8 off-policyness performed equally well, and we adopt 8 as the default setting when updating the
baseline in Section 3.1.
Why does PipelineRL consistently outperform the classic PPO-off-policy approach (Sections 3.1
and 3.3)? We attribute this to its closer alignment with on-policy training. In PPO-off-policy, gen-
eration and training proceed in alternating phases: the trainer operates strictly on batches that are as
off-policy as the chosen parameter k, making updates based on stale rollouts. In contrast, Pipelin-
eRL operates in a streaming fashion. As soon as a batch is available, it is passed to the trainer;
likewise, as soon as a model update is ready, it is shared back to the generators, who immediately
use it—including in the continuation of partially generated traces. This tight feedback loop keeps
training closer to the on-policy regime, reducing the mismatch between generator and trainer distri-
butions.
Importantly, this distinction affects the asymptotic performance c of the scaling curve, not just the
efficiency exponent b. Very few axes shift the asymptote in this way, making the choice of off-policy
algorithm one of the most consequential design decisions in RL post-training.
(a)
Figure 9: Different off-policy runs with PipelineRL
17
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
946
947
948
949
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
Under review as a conference paper at ICLR 2026
A.10 GSPO ABLATIONS
We ablate the clipping-ratio scale used in GSPO, as shown in Figure 10a. The 10−3 scale consis-
tently performs as well as, or better than, alternatives. Given this scale, we further varied the upper
clipping ratio in {4×10−3,5×10−3,7×10−3}and found {5×10−3}yielded the at least as good fit
as the other choices.
An important observation is that GSPO is quite robust to the choice of clipping ratio. Once the
correct scale is identified, most nearby values or even larger scale perform similarly. This robustness
contrasts sharply with DAPO-style losses, which are highly sensitive to the exact value of the higher
clipping ratio, as noted in Section 3.2.
(a)
Figure 10: GSPO Scale comparison. gspo x y e z in the legend means an upper and lower threshold
of {x ×10−z and y ×10−z}respectively.
A.11 CISPO CLIPPING RATIOS
We ablate the higher clipping ratio for CISPO, keeping the lower clipping ratio fixed at 0 (Fig-
ure 11a). Across a wide range of values, we find little difference in performance, indicating that
CISPO is largely insensitive to this hyperparameter. This robustness mirrors our findings for GSPO
(Section A.10), and stands in contrast to DAPO/GRPO-style objectives, which are highly sensitive to
the exact choice of clipping threshold. Such stability under hyperparameter variation makes CISPO
a strong candidate for default use in large-scale training.
A.12 ENTROPY
We tracked entropy on the held-out in-distribution evaluation set throughout training. Across all
experiments—spanning variations in batch size, number of tasks, generation length, and model
scale—we observed a consistent overall decrease in entropy. We show this in Figure Figure 12a
For brevity, we report only the most representative and insightful plots here.
In Figure 12a, we plot entropy for ScaleRL runs with batch sizes 768 and 2048. Despite the 2048-
batch size run achieving much stronger downstream performance at every stage (Figure 6c), both
runs followed nearly identical entropy trajectories per step. This highlights an important point -
although entropy is sometimes used as a proxy for exploration, simply maintaining higher entropy
does not translate into better generalization. Instead, larger batches reduced effective exploration
yet still yielded substantially better performance, underscoring batch size as an important decisive
factor.
18
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
Under review as a conference paper at ICLR 2026
(a)
Figure 11: CISPO clipping ratio ablations
(a)
Figure 12: (a) Comparing entropy of large and smaller batch size runs
Overall, our findings suggest that while entropy decreases consistently during training, it is not by
itself a reliable predictor of downstream performance. This observation reinforces the need to focus
on algorithmic and scaling choices (e.g., batch size, off-policy method) rather than entropy dynamics
when aiming for improved performance, both on training distribution as well as downstream task
distribution.
A.13 SCALING ON MULTIPLE AXES
We provide the remaining scaling to different axes figure here in Figure 13
A.14 DOWNSTREAM PERFORMANCE
In Figures 6a to 6c, we report a representative set of downstream evaluation curves. These in-
clude ScaleRL runs with batch sizes {512,768,2048}, long-context training run with 32k generation
length, the large-model (Scout) training run, and a multi-task run (math + code). For each setting
we plot performance against compute, and in some cases also against training steps for comparabil-
19
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
Under review as a conference paper at ICLR 2026
(a) (b) (c)
Figure 13: Scaling to (a) different number of generationes per prompt, (b) Code performance on
math+code run, (c) Math performance on math+code run
ity across runs of different scales (to avoid smaller runs being visually compressed on the compute
axis).
Two main patterns emerge. First, smaller-batch runs show early stagnation on downstream bench-
marks even as in-distribution performance continues to improve. Second, larger-batch runs avoid
this stagnation and instead exhibit downstream scaling curves that mirror their in-distribution power-
law behavior. This supports the conclusion that larger batch sizes should be preferred for stable and
scalable downstream performance.
A.15 TRUNCATIONS AND TRAINING INSTABILITIES
Across our experiments we found that training instabilities were often linked to truncations. As gen-
eration length grew, many RL runs exhibited fluctuating truncation rates that sometimes increased
over training. At batch size 768, we observed that truncations in the range of 10–15% typically
destabilized training, with performance degrading and not recovering without intervention. Exam-
ples include the extended GRPO run in Figure 2, where instability correlated with rising truncation
rates, and the updated baseline used in Section 3.2.
By contrast, ScaleRL runs were more stable. On the 8B model, truncations remained below 5% for
over 90% of training. At batch size 2048, truncations were slightly higher, occasionally approaching
∼ 7%. This increase was largely attributable to longer average generation lengths observed during
training, which naturally raise the chance of exceeding the budget. Nevertheless, because the effec-
tive batch size (after excluding truncated samples) remained large, training stability was preserved.
Intuitively, larger generation length budget should help reduce truncations. Training with 34k gener-
ation length (batch 768) remained stable - truncations briefly spiked to ∼4% but quickly fell below
2%.
Larger models were even more robust. On the Scout run, truncations remained consistently below
2%, and for > 90% of training steps were under 1%. This likely reflects both the inherent ability
of larger models to regulate generation length and their stronger instruction-following ability, which
made interruption signals more effective.
Overall, we suggest practitioners monitor truncation rates closely. Our findings indicate that high
truncation rates are a reliable warning signal of instability, while larger models, higher generation
budgets, and careful design choices (as in ScaleRL) substantially mitigate this risk.
A.16 LOSS TYPE - STABILITY AND ROBUSTNESS
As discussed in Section 3.2, GRPO/DAPO-style losses are highly sensitive to the choice of clipping
ratio hyperparameter εmax. In contrast, CISPO and GSPO show far greater robustness. For exam-
ple, in Appendix Section A.11, varying εmax for CISPO between {4,5,8}produced no significant
differences in performance. For GSPO, the 10−4 clipping scale used in the original paper (Zheng
et al., 2025a) did not work well in our setting. We therefore ablated across broader scales and found
that once the correct order of magnitude was identified (e.g., 4×10−3 and higher), performance was
stable and largely insensitive to fine-grained changes (e.g., {4×10−3,5×10−3,7×10−3}).
20
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
Under review as a conference paper at ICLR 2026
That said, we encountered additional stability issues with GSPO. On multiple occasions, GSPO runs
diverged mid-training, leading to sudden drops in performance. For 8B models, restarting from a
stable checkpoint allowed recovery, but this strategy failed on larger models such as Scout, where
instability persisted despite repeated resetting to a stable checkpoint. While we checked to the best
of our ability for any implementation bugs, we did not find one.
Overall, while all three loss families can be competitive under tuned settings, CISPO offers the best
balance of stability and robustness to hyperparameters, making it our recommended choice.
21