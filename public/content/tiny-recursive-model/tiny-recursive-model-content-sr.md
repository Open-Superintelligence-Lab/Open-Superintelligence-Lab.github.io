---
hero:
  title: "Sitni Rekurzivni Model"
  subtitle: "Nova AI arhitektura rekurzivnog rasuÄ‘ivanja"
  tags:
    - "â±ï¸ TehniÄki Dubinski Prikaz"
    - "ğŸ“„ IstraÅ¾ivaÄki ÄŒlanak"
---

## Nova AI Arhitektura RasuÄ‘ivanja

Kako model od 7M pobedi modele od 1T u Sudoku, Lavirintima, ARC-AGI

**Sitni Model RasuÄ‘ivanja (TRM)** koristi 2-slojni transformer (7M parametara) koji ponovo koristi iste slojeve stotine puta da razmiÅ¡lja o problemima rekurzivno.

Pobedi 100x veÄ‡e modele u Sudoku-Extreme, Lavirintima, ARC-AGI i joÅ¡ mnogo toga.

U ovom tutorijalu Ä‡emo nauÄiti kako TRM funkcioniÅ¡e i uraditi sopstvene eksperimente.

---

## TRM Arhitektura - Pregled

![Arhitektura Sitnog Rekurzivnog Modela](/content/tiny-recursive-model/images/tiny-recursive-model-architecture.png)
*Slika: Arhitektura Sitnog Rekurzivnog Modela koja prikazuje glavni blok obrade (4x transformer slojeva), kombinaciju ulaza pitanja (x), odgovora (y) i rasuÄ‘ivanja (z), obradu izlaza za raÄunanje gubitka, i mehanizam rekurzivnog aÅ¾uriranja koji iterativno poboljÅ¡ava rasuÄ‘ivanje i predviÄ‘anje kroz maksimalno 16 koraka.*

Dijagram iznad ilustruje kompletnu TRM arhitekturu. Model obraÄ‘uje tri kljuÄne komponente:
- **Ulaz (x)**: Pitanje ili problem za reÅ¡avanje (npr. raspored lavirinta)
- **PredviÄ‘anje (y)**: Trenutni pokuÅ¡aj odgovora modela
- **Latentno (z)**: Interno stanje rasuÄ‘ivanja modela

Ove komponente se kombinuju i obraÄ‘uju kroz stek od 4 transformer sloja, sa izlazom koji se koristi za raÄunanje cross-entropy gubitka. KljuÄna inovacija je mehanizam rekurzivnog aÅ¾uriranja na dnu, koji iterativno poboljÅ¡ava i rasuÄ‘ivanje (z) i predviÄ‘anje (y) kroz viÅ¡e koraka da postepeno poboljÅ¡a reÅ¡enje.

---

## Kako TRM FunkcioniÅ¡e

### Korak 1: Postavljanje

Hajde da treniramo TRM da reÅ¡i lavirint.

**1. Predstavljanje Lavirinta kao MreÅ¾e:**
Prvo, prikazujemo lavirint kao mreÅ¾u brojeva. Svaka Ä‡elija u mreÅ¾i dobija broj.

-   `0` = Prazan put
-   `1` = Zid
-   `2` = PoÄetna taÄka
-   `3` = Krajnja taÄka

Za konkretni primer, hajde da pratimo mali 3x3 lavirint.

-   **`x_input`** (NereÅ¡en lavirint)
    ```
    [[2, 0, 1],
     [1, 0, 1],
     [1, 0, 3]]
    ```
-   **`y_true`** (Ispravno reÅ¡enje, sa `4` koje predstavlja putanju)
    ```
    [[2, 4, 1],
     [1, 4, 1],
     [1, 4, 3]]
    ```

**2. Tokenizacija:**
Termin **token** jednostavno znaÄi jednu jedinicu naÅ¡ih podataka. U ovom sluÄaju, jedan broj u mreÅ¾i (`0`, `1`, `2`, `3` ili `4`) je token. Da bismo olakÅ¡ali mreÅ¾u za obradu, "odmotavamo" mreÅ¾u u dugu 1D listu.

Za naÅ¡ 3x3 primer, mreÅ¾a se odmotava u listu od 9 tokena.

**3. UgraÄ‘ivanje: Davanje ZnaÄenja Brojevima:**
Da bi model razumeo Å¡ta brojevi poput `4` i `1` znaÄe, dodeliÄ‡emo veliki **vektorski embedding** svakom. Vektorski embedding je dugaÄak vektor (niz brojeva) koji model moÅ¾e menjati da Äuva informacije o zidu, praznom putu, itd.

Ovi vektori Ä‡e predstavljati znaÄenje "zida" ili "krajnje taÄke".

PreporuÄujem da podsetite sebe Å¡ta su vektorski embeddingovi (u LLM-ovima, reÄi, tokena, itd) pretraÅ¾ivanjem na YouTube-u ili razgovorom sa AI chatbotom.

-   **Sloj UgraÄ‘ivanja** je kao reÄnik.
-   SadrÅ¾i vektorske embeddingove za svaki od naÅ¡ih brojeva.
-   `1`: `[0.3, -1.2, 0.7, 0.0, 1.5, -0.4, 0.9, 2.3]`  â†  Primer vektorskog embeddinga za "zid"
-   **Izlaz:** Duga lista brojeva nazvana **vektor**. Ovaj vektor predstavlja *znaÄenje* "zida" na naÄin koji mreÅ¾a moÅ¾e razumeti. Sama mreÅ¾a bira (nauÄi) brojeve unutar ovog vektora tokom treninga tako da ga moÅ¾e "razumeti".

Nakon ovog koraka, naÅ¡ ulazni lavirint viÅ¡e nije lista jednostavnih brojeva. To je lista vektora. Za naÅ¡ 3x3 lavirint, ako koristimo vektor veliÄine 8 za svaki token, naÅ¡ ulaz postaje:

-   `x`: Matrica `9x8` vektora koja predstavlja lavirint.

Ova bogata reprezentacija je ono Å¡to unosimo u glavni model.

---

### Korak 2: Osnovna Arhitektura: TRM Mozak

"Mozak" TRM-a je mali 2-slojni transformer nazvan `net`. On obraÄ‘uje informacije da bi proizveo izlaz. Da bi "razmiÅ¡ljao", TRM koristi dve promenljive, obe istog oblika kao `x`:

-   `y`: Trenutna **najbolja pretpostavka** modela za reÅ¡enje. MoÅ¾e biti pogreÅ¡na
```
[[2, 4, 1],
  [1, 4, 1],
  [1, 0, 3]]
```
-   `z`: **Latentna misao**. `z` govori Å¡ta treba promeniti u `y` da bi se pretvorilo u ispravno reÅ¡enje. `z` se propuÅ¡ta kroz transformer viÅ¡e puta da bi model poboljÅ¡ao Å¡ta treba promeniti u `y`, tako model rasuÄ‘uje ili razmiÅ¡lja. Zatim se promena primenjuje na `y`.

Za naÅ¡ 3x3 primer, `z` i `y` poÄinju kao `9x8` matrice nula.

---

### Korak 3: Proces UÄenja, Iznutra ka Spolja

TRM uÄi kroz seriju ugneÅ¾denih petlji. PoÄnimo od jezgra i gradimo naÅ¡ put ka spolja.

#### Najdublja Petlja: `latent_recursion` (Osnovna Misao)

Ovde mali `net` (2-slojni Transformer) obavlja sav svoj posao. Proces je podeljen u dve faze koje se ponavljaju formirajuÄ‡i ciklus razmiÅ¡ljanja i poboljÅ¡anja.

**Faza A: RasuÄ‘ivanje (AÅ¾uriranje SkraÄ‡enice `z`)**
Model "razmiÅ¡lja" poboljÅ¡avajuÄ‡i svoj interni token planiranja `z`, u petlji od 6 koraka. Cilj je izgraditi sve bolji plan za promenu `y`.

1.  **Proces:** U svakom od 6 koraka, `net` prima tri ulaza:
    -   Sam lavirint (`x`).
    -   Trenutnu najbolju pretpostavku modela za reÅ¡enje (`y`) - ovo moÅ¾e biti sve nule na poÄetku.
    -   SkraÄ‡enicu iz prethodnog koraka (`z`).
2.  **Kako radi:**
    -   **Kombinovanje Ulaza:** Tri ulaza se sabiraju element po element (`x + y + z`). Ovo kreira jedan niz bogatih vektora, gde svaki vektor (koji predstavlja Ä‡eliju u lavirintu) sadrÅ¾i kombinovane informacije o rasporedu lavirinta (`x`), trenutnoj pretpostavci (`y`), i procesu trenutne misli (`z`).
    -   **RazmiÅ¡ljanje sa PaÅ¾njom:** Ovaj kombinovani niz se ubacuje u 2-slojni Transformer. Mehanizam samo-paÅ¾nje Transformera omoguÄ‡ava mu da pogleda sve Ä‡elije odjednom i identifikuje odnose. Na primer, moÅ¾e videti kako "poÄetna" Ä‡elija ima odnos prema potencijalnoj Ä‡eliji putanje, informisan ulaznim podacima `x` i rasuÄ‘ivanjem `z`.
    -   **Generisanje SledeÄ‡e Misli:** Dva transformer sloja obraÄ‘uju ovu informaciju i ispisuju novi niz vektora identiÄnog oblika. Ovaj izlaz *je* novo `z`. Ne postoji zaseban "izlazni glava" da ga generiÅ¡e; transformacija koju obavljaju dva sloja *je* Äin kreiranja sledeÄ‡e, poboljÅ¡ane misli. Iako je ulaz bio zbir koji sadrÅ¾i `x` i `y`, mreÅ¾a uÄi da proizvede izlaz koji sluÅ¾i kao korisno novo `z` za sledeÄ‡i korak.

    Ovaj proces se ponavlja 6 puta, Å¡to znaÄi da se informacija propuÅ¡ta kroz ista dva sloja Å¡est uzastopnih puta, postajuÄ‡i progresivno sofisticirana sa svakim prolaskom.
3.  **Primer Toka:** Nakon nekoliko prolaza kroz transformer, `z` moÅ¾e kodirati nisko-nivovske karakteristike poput lokacija zidova. Do Å¡estog prolaza, moÅ¾e predstavljati visoko-nivo plan za aÅ¾uriranje odgovora (`y`).
   
   - Interesantno, ista 2 transformer sloja se koriste za detekciju nisko nivoskih karakteristika, pravljenje visoko nivoskog plana i kasnije za aÅ¾uriranje same `y`. Ova 2 sloja imaju viÅ¡estruke namene, Å¡to je moÄ‡ neuronskih mreÅ¾a, moÅ¾e nauÄiti da obavi viÅ¡estruke, manje povezane ili nepovezane transformacije koje zavise samo od ulaznih podataka.

**Faza B: PoboljÅ¡anje Odgovora (AÅ¾uriranje Pretpostavke `y`)**
Nakon petlje rasuÄ‘ivanja od 6 koraka, koristeÄ‡i najnoviju latentnu misao `z`, model aÅ¾urira svoj odgovor `y`.

-   **Kako radi:** Kombinuje svoj prethodni odgovor (`y`) sa svojom finalnom, poboljÅ¡anom miÅ¡lju (`z`) sabrajuÄ‡i ih (`y + z`) i propuÅ¡ta rezultat kroz isti `net` joÅ¡ jednom. Izlaz je novo, poboljÅ¡ano `y`.
    -   **KljuÄno, `x` nije ukljuÄeno u ovom koraku.** Ovo je namerna odluka dizajna koja govori jednom `net` koji zadatak treba obaviti.
    -   `x` je prisutno u rasuÄ‘ivanju (`x + y + z`).
    -   `x` nedostaje u poboljÅ¡anju odgovora (`y + z`).

Razlog zaÅ¡to sam rekao "poboljÅ¡anje odgovora" je zato Å¡to se ova 6+1 petlja deÅ¡ava viÅ¡e puta, svaki put "razmiÅ¡ljajuÄ‡i" 6 prolaza i aÅ¾urirajuÄ‡i `y` jednom.

#### Srednja Petlja: `deep_recursion` (Potpun Proces RazmiÅ¡ljanja)

Sada kada razumemo kako rasuÄ‘ivanje + petlja poboljÅ¡anja y radi, hajde da vidimo potpun proces razmiÅ¡ljanja od poÄetka gde se cela ova petlja ponavlja 3 puta da bi se dobilo najbolje `y`.

Prethodno opisana unutraÅ¡nja petlja (6+1 koraka rasuÄ‘ivanja i poboljÅ¡anja `y`) se izvrÅ¡ava `T` puta (npr., `T=3`). Stanje (`y` i `z`) se **prenosi** izmeÄ‘u ovih izvrÅ¡avanja; ne resetuje se na nulu.

-   **Runda 1 (Zagrevanje):** PoÄinje sa praznim (sve nule) `y` i `z` (zapamtite, ovo je apsolutni poÄetak procesa, tako da nema `y` i `z` za prenoÅ¡enje). IzvrÅ¡ava potpunu unutraÅ¡nju petlju (6 rasuÄ‘ivanja + 1 poboljÅ¡anje `y` koraka) da proizvede pametnije `y_1` i `z_1`. Ovo se radi u "bez-gradijent" modu za brzinu i uÅ¡tede memorije - neuronska mreÅ¾a ne uÄi ovde.
-   **Runda 2 (Zagrevanje):** Uzima `y_1` i `z_1` kao poÄetnu taÄku i izvrÅ¡ava unutraÅ¡nju petlju ponovo da proizvede joÅ¡ bolje `y_2` i `z_2`. JoÅ¡ uvek nema gradijenata i uÄenja.
-   **Runda 3 (Zaista):** PoÄinje sa dobro razmotrenim `y_2` i `z_2`, izvrÅ¡ava unutraÅ¡nju petlju joÅ¡ jednom, i ovaj put svi proraÄuni se prate tako da model moÅ¾e nauÄiti sa propagacijom unazad.

Ovaj proces zagrevanja modelove "misli" pre finalnog, uÄivog koraka je kljuÄna optimizacija.

#### Najspoljnija Petlja: JoÅ¡ viÅ¡e petlji!

Model dobija viÅ¡estruke "Å¡anse" (do 16) da reÅ¡i isti lavirint, i nakon svake Å¡anse, poboljÅ¡ava svoje `net` teÅ¾ine. Stanje (`y` i `z`) **se prenosi** sa jedne iteracije srednje petlje na sledeÄ‡u, kao Å¡to je prikazano u pseudokodu rada. Dozvoljava modelu da dobije viÅ¡estruke "Å¡anse" (do 16) da reÅ¡i isti lavirint, poboljÅ¡avajuÄ‡i se sa svakom.

Ovo je samo ponavljanje srednje petlje do 16 puta. Model moÅ¾e odluÄiti da se zaustavi ranije od 16 ako oseÄ‡a da je dobio ispravan odgovor.

ZaÅ¡to nam treba ova petlja:

Nakon svake iteracije srednje petlje, ova spoljaÅ¡nja petlja aÅ¾urira teÅ¾ine jednom (zapamtite da Runda 3 u srednjoj petlji radi propagaciju unazad).

Zatim u sledeÄ‡oj iteraciji ponavlja srednju petlju sa aÅ¾uriranim teÅ¾inama, dopuÅ¡tajuÄ‡i modelu da postepeno poboljÅ¡a svoje reÅ¡enje sa svakim pokuÅ¡ajem.

#### Znanje kada prestati da razmiÅ¡lja (Q glava)

SpoljaÅ¡nja petlja moÅ¾e raditi do 16 puta, ali ne mora. Bilo bi gubljenje vremena nastaviti razmiÅ¡ljati o lavirintu koji je veÄ‡ reÅ¡en.

Dakle, model ima mali sporedni mozak nazvan "Q glava". Nakon svakog potpunog procesa razmiÅ¡ljanja (svake srednje petlje), ova Q glava izbacuje rezultat. Ovaj rezultat je u osnovi modelovo poverenje: "Koliko sam siguran da sam ovo dobio taÄno?"

Ako je rezultat poverenja dovoljno visok, spoljaÅ¡nja petlja se samo zaustavlja (`break`), i model prelazi na sledeÄ‡i lavirint.

UÄi da dobije ovaj rezultat poverenja taÄno jer je to deo treninga. NagraÄ‘uje se ako je siguran *i* taÄan, i kaÅ¾njava se ako je siguran ali pogreÅ¡an. Rad ovo naziva Adaptivno Vreme RaÄunanja (ACT).

---

```python
# Inicijalizacija
y, z = zeros_like(x), zeros_like(x)

# Petlja dubinske supervizije (do 16 puta)
for supervision_step in range(16):
    
    # Dubinska rekurzija: zagrevanje (2 puta, bez gradijenata)
    with torch.no_grad():
        for _ in range(2):
            # Latentna rekurzija
            for _ in range(6):
                z = net(x + y + z)
            y = net(y + z)
    
    # Dubinska rekurzija: finalni (1 put, SA gradijentima)
    for _ in range(6):
        z = net(x + y + z)
    y = net(y + z)
    
    # UÄenje
    y_pred = output_head(y)
    loss = cross_entropy(y_pred, y_true)
    loss.backward()
    optimizer.step()
    
    # Da li bi trebalo da se zaustavimo?
    q = Q_head(y)
    if q > 0:
        break
```

---

### Korak 4: Studije Ablacije - Å ta TRM ÄŒini Funkcionalnim?

![Kompletan Studija Ablacije](/content/tiny-recursive-model/images/complete_ablation_study.png)
*Slika: PoreÄ‘enje gubitka treninga preko Äetiri TRM konfiguracije tokom 10 epoha na reÅ¡avanju lavirinta (30x30, teÅ¡ko). Osnovna linija (plava puna) koristi TRM standardni dizajn: 2-slojna mreÅ¾a, H=3 (srednja petlja), L=6 (unutraÅ¡nja petlja), sa EMA. Ablacije testiraju: uklanjanje EMA (crvena isprekidana), smanjenje dubine rekurzije (zelena taÄka-crta), i koriÅ¡Ä‡enje veÄ‡e 4-slojne mreÅ¾e (magenta taÄkasta).*

Da bismo razumeli Å¡ta Äini TRM efikasnim, sistematski testiramo varijacije uklanjanjem ili menjanjem kljuÄnih komponenti. Ove **studije ablacije** otkrivaju koje odluke dizajna su esencijalne.

#### Eksperimentalno Postavljanje

Testiramo Äetiri konfiguracije na zadatku reÅ¡avanja lavirinta (30x30 teÅ¡ki lavirinti, 1000 trening primera):

| Konfiguracija | Slojevi | H_ciklusi | L_ciklusi | EMA | Efektivna Dubina* |
|---------------|---------|-----------|-----------|-----|-------------------|
| **Osnovna TRM** | 2 | 3 | 6 | Da | 42 |
| **Bez EMA** | 2 | 3 | 6 | Ne | 42 |
| **Manje Rekurzije** | 2 | 2 | 2 | Da | 12 |
| **VeÄ‡i Mozak** | 4 | 3 | 3 | Da | 48 |

*Efektivna dubina = T Ã— (n+1) Ã— slojevi

#### Rezultati

**Napomena:** Ovo su 10-epoha eksperimentiâ€”veoma mala koliÄina treninga u poreÄ‘enju sa 50,000+ epoha izvrÅ¡avanjima rada. DuÅ¾e treniranje moÅ¾e znaÄajno promeniti relativnu performansu ovih konfiguracija, naroÄito za generalizaciju (kao Å¡to vidimo sa "VeÄ‡i Mozak" rezultatima ispod).

| Konfiguracija | PoÄetni Gubitak | Finalni Gubitak | Min Gubitak | PoboljÅ¡anje |
|---------------|-----------------|-----------------|-------------|-------------|
| Osnovna | 1.789 | 1.062 | 1.045 | 40.6% |
| Bez EMA | 1.789 | 1.042 | 1.041 | 41.7% |
| Manje Rekurzije | **2.100** | 1.100 | 1.042 | 47.6% |
| VeÄ‡i Mozak (4-slojni) | 1.789 | **1.007** | **1.007** | **43.7%** |

#### KljuÄni Nalazi

**1. Paradoks "VeÄ‡eg Mozga": KratkoroÄna vs. DugoroÄna Performansa**

4-slojna mreÅ¾a je postigla **najbolji finalni gubitak** (1.007) u naÅ¡im 10-epoha eksperimentima, nadmaÅ¡ujuÄ‡i 2-slojnu osnovnu liniju za ~5%. Ovo izgleda da protivreÄi tvrdnji rada da "manje je viÅ¡e".

**ZaÅ¡to Razlika?**
- **KratkoroÄno** (10 epoha): ViÅ¡e kapaciteta = brÅ¾e uÄenje. 4-slojna mreÅ¾a moÅ¾e brzo memorisati obrasce.
- **DugoroÄno** (50k+ epoha): ViÅ¡e kapaciteta = prekomerno prilagoÄ‘avanje. 2-slojna mreÅ¾a je *primorana* da nauÄi ponovne strategije rasuÄ‘ivanja umesto memorisanja specifiÄnih reÅ¡enja.
  
Osnovna teza rada: **Male mreÅ¾e primorane da razmiÅ¡ljaju rekurzivno generalizuju bolje od velikih mreÅ¾a**, Äak i ako se treniraju sporije inicijalno. 2-slojna arhitektura je izabrana specifiÄno da spreÄi memorisanje i prisili oslanjanje na rekurziju.

**2. Dubina Rekurzije je Fundamentalna**

Konfiguracija "Manje Rekurzije" (H=2, L=2) pokazuje ozbiljno degradiranu performansu:
- PoÄela na **17% viÅ¡em poÄetnom gubitku** (2.100 vs 1.789) pre bilo kakvog treninga
- Postigla najgori finalni gubitak (1.100) uprkos poboljÅ¡anju od 47.6%

**Å ta Rad KaÅ¾e:** Smanjenje rekurzije sa T=3, n=6 na T=2, n=2 smanjuje Sudoku taÄnost sa 87.4% na 73.7% â€” pad od 14%.

**ZaÅ¡to Ovo Ima ZnaÄaj:** Visok poÄetni gubitak otkriva da plitka rekurzija sakati modelovu reprezentacionu moÄ‡ *po dizajnu*. ÄŒak i sa perfektnim treningom, nema dovoljno rekurzivnih "koraka razmiÅ¡ljanja" da reÅ¡i kompleksne probleme. **Ne moÅ¾ete kompenzovati nedovoljnu dubinu rekurzije sa boljim treningom.**

**3. EMA Ima Minimalan KratkoroÄan Uticaj**

Uklanjanje EMA jedva je uticalo na 10-epoha performansu (finalni gubitak 1.042 vs 1.062 za osnovnu liniju, samo ~2% razlika).

**Å ta Rad KaÅ¾e:** Na Sudoku-Extreme, uklanjanje EMA smanjuje taÄnost sa 87.4% na 79.9% â€” pad od 8% nakon potpunog treninga.

**ZaÅ¡to Razlika?** EMA je **Eksponencijalni PomiÄni Prosek** teÅ¾ina modela koji stabilizuje trening tokom dugih izvrÅ¡avanja. U kratkim eksperimentima, oba modela joÅ¡ uvek istraÅ¾uju i nisu joÅ¡ susreli nestabilnost koju EMA spreÄava. Tokom 50,000+ epoha, EMA spreÄava katastrofiÄnu divergenciju i pikove prekomerne prilagoÄ‘enosti, ÄineÄ‡i ga esencijalnim za finalnu performansu.

---

Hvala vam Å¡to ste proÄitali ovaj tutorijal i vidimo se u sledeÄ‡em.

